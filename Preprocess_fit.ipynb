{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file first preprocesses the text files and loads them into a numpy array ready for modeling, then fits Naive Bayes and an SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import operator as op\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parties = ['act', 'green', 'labour', 'maori', 'national', 'nzfirst']\n",
    "\n",
    "strip_list = ['Posted by', '\\n', 'Jacinda', 'Ardern', 'Steven', 'Joyce', ' Bill ', 'English', 'Carmel', 'Sepuloni', 'Barry',\n",
    "                '2017', 'David', 'Clark', 'Phil', 'Twyford', 'Michael', 'Wood', 'Chris ', 'Hipkins', 'Grant', 'Maggie',\n",
    "                'Robertson', 'Greg', 'O’Connor', 'Andrew', ' Little', 'Winston', 'Peters', 'Damien', 'O\\'Connor',\n",
    "                'Kelvin', 'Davis', 'Phil', 'Twyford', 'Megan', 'Woods', 'Parker', 'Nanaia', 'Mahuta', 'Paula', 'Bennett',\n",
    "                'Carter', 'Gerry', 'Brownlee', 'Simon', ' Bridges', ' Amy', 'Adams', 'Jonathan', 'Coleman', 'Christopher',\n",
    "                'Finlayson', 'Woodhouse', 'Nathan', 'Guy', 'Anne', 'Tolley', ' Ron ', 'Mark', 'Marama', 'Fox', ' Te ', \n",
    "                'Ururoa', 'Flavel', 'Jones', 'Shane', 'Taurima', 'Seymour', 'James', 'Shaw', 'Marama', 'Davidson', ' Dr ',\n",
    "                'Julie', 'Anne', 'Genter', 'Jan ', 'Logie', 'Eugenie', 'Sage', 'Gareth', 'Hughes', 'Steffan', 'Browning',\n",
    "                'Rt', 'Hon', 'Nick', 'Smith', 'Nikki', 'Kaye', 'Nicky', 'Wagner', 'Minister', 'Paul', 'Goldsmith',\n",
    "                'ACT', ' National ', 'Green Party', 'Labour', 'First ', 'ENDS', '.', ',' '\\\"', '\\'', 'Māori Party',\n",
    "                '“','”', 'Facebook5Twitter', 'Steffan', 'Browning', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                'Carlton', 'Burke', 'Chadwick', 'Catherine', 'Christine', 'Alex', 'Alexander', 'Baker', 'Carolyn', \n",
    "                'Alyssa', 'Brown', 'Bob', 'Byrn', 'Augustine', 'Crawford', 'Antonio', 'Claudetta', 'Christina', 'Collins'\n",
    "                'Buckner', 'centre', 'Ben', 'Boyden',  'Alan', 'Bosley', '’', 'Alastair', 'Ballantyne', 'Bruno', 'Cecelia',\n",
    "                 'Allan', 'Bernard', 'Anderson', 'Andrea', 'Tim', 'spokesperson', 'Scott', 'Simpson', 'Epsom', 'Metiria',\n",
    "                 'Turei', 'said', 'say', 'John Key', 'John', 'Tukoroirangi Morgan', 'Dame', 'Mei', 'Reedy', 'Leader', \n",
    "                 'Northland', 'Member of Parliament', 'Spokesperson', 'Don', 'Houlbrook', 'Stephen', 'Todd', 'Barclay',\n",
    "                 'Morgan', 'Tariana', 'Turia', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday',\n",
    "                  'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', \n",
    "                  'November', 'December', 'Southland', 'Judith', 'Collins', 'Jacqui', 'Dean', 'Bhupind', 'Singh', 'van Velden',\n",
    "                 'govt.nz', 'Mitchel', 'New Zealand', 'First', 'NZ', 'Mitchell', 'Tracy', 'Martin', 'Mike', 'MP', 'Prosser',\n",
    "                 'William', 'Sio', 'Don', 'Zealand', 'Aupito', 'Kevin', 'Hague', 'Bhupind', 'Singh', 'Louise', 'Upston'] \n",
    "\n",
    "strip_from_stemmed = ['conclusionth', 'bennet', 'brydon', 'bosley', 'centrewellington', 'countrynew', 'ballantyn', 'allan',\n",
    "                     'delahunti', ',', 'beth', 'ms', 'mr', 'www', 'http', 'media contact', 'govt', 'nz'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_in(party, folder):\n",
    "    release_text_list = []\n",
    "    for filename in os.listdir(folder):\n",
    "        full_path = os.path.join(party, filename)\n",
    "        file_obj = open(os.path.join(party, filename), 'r', encoding='utf8')\n",
    "        content = file_obj.read()\n",
    "        file_obj.close()\n",
    "        release_text_list.append(content)\n",
    "    return(release_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary where key = label (party) and value = list of release text strings.\n",
    "\n",
    "dict_of_text_lists = {}\n",
    "list_of_all_texts = []\n",
    "list_of_all_party_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    list_of_all_party_texts = read_in(party, party)\n",
    "    dict_of_text_lists[party] = list_of_all_party_texts\n",
    "    print('Total docs for party', party, '=', len(list_of_all_party_texts))\n",
    "    list_of_all_texts = list_of_all_texts + list_of_all_party_texts\n",
    "    list_of_all_party_texts = []\n",
    "\n",
    "print('Total documents: ', len(list_of_all_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Undersample National to improve dataset balance\n",
    "\n",
    "print ('Removing three out of every four National Party press release over time:')\n",
    "print('National had', len(dict_of_text_lists['national']), 'press releases')\n",
    "top_end = len(dict_of_text_lists['national'])\n",
    "              \n",
    "for i in range(0, int(top_end/4)):\n",
    "#for i in range(1, int(top_end/4)):  # Robustness check - try a different quarter\n",
    "#for i in range(2, int(top_end/4)):  # Robustness check - try a different quarter\n",
    "#for i in range(3, int(top_end/4)):  # Robustness check - try a different quarter\n",
    "\n",
    "    del dict_of_text_lists['national'][i:i+3]\n",
    "    i += 3\n",
    "    \n",
    "print('National now has', len(dict_of_text_lists['national']), 'press releases')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dict_of_text_lists['green'][30][:150]) # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get rid of names (giveaways for the authoring problem) and a few other 'cheat' strings, and stem the text\n",
    "\n",
    "stem_words = []\n",
    "party_list_of_proc_texts = []\n",
    "dict_of_proc_text_lists = {}\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "for party in parties:\n",
    "    party_list_of_proc_texts = []\n",
    "    \n",
    "    # Remove the words in my manual strip list above\n",
    "    for text in dict_of_text_lists[party]:\n",
    "        strip_text = text\n",
    "        for goner in strip_list:\n",
    "            if goner in text:\n",
    "                strip_text = strip_text.replace(goner, ' ')                \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # Apply Snowball stemmer:\n",
    "        stem_words = []\n",
    "        text_words = strip_text.split()\n",
    "        for word in text_words:\n",
    "            stem_word = stemmer.stem(word)\n",
    "            stem_words.append(stem_word)\n",
    "        stem_text = \" \".join(stem_words)\n",
    "        \n",
    "        # Strip a few problematic strings from the stemmed text:\n",
    "        for stemword in strip_from_stemmed:\n",
    "            if stemword in stem_text:\n",
    "                stem_text = stem_text.replace(stemword, ' ')\n",
    "        party_list_of_proc_texts.append(stem_text)\n",
    "        \n",
    "    # Put list of processed party texts in dictionary with party as key\n",
    "    dict_of_proc_text_lists[party] = party_list_of_proc_texts\n",
    "\n",
    "print('Example:')    \n",
    "print('Original text:', text[:80])\n",
    "print('\\n', 'Stripped text:', strip_text[:80])\n",
    "print('\\n', 'After stem:' , stem_text[:80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test a few examples\n",
    "print(dict_of_proc_text_lists['act'][10][0:100])\n",
    "print(dict_of_proc_text_lists['labour'][3][0:100])\n",
    "print(dict_of_proc_text_lists['green'][53][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a list of (unlabelled) processed texts\n",
    "list_of_all_proc_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    procd = dict_of_proc_text_lists[party]\n",
    "    list_of_all_proc_texts = list_of_all_proc_texts + procd\n",
    "    \n",
    "print(len(list_of_all_proc_texts))\n",
    "    \n",
    "# Make a list of party authors that will match up with the texts \n",
    "party_match = []\n",
    "for party in dict_of_proc_text_lists: # 6 parties\n",
    "    for text in dict_of_proc_text_lists[party]:\n",
    "        party_match.append(party)\n",
    "        \n",
    "print(len(party_match))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Text is ready for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into training and testing sets:\n",
    "docs_train, docs_test, labels_train, labels_test = model_selection.train_test_split(list_of_all_proc_texts, \n",
    "                                                                                    party_match, \n",
    "                                                                                    test_size = 0.2,\n",
    "                                                                                    stratify = party_match)\n",
    "# Stratifying ensures same balance of test as training data\n",
    "print(len(docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text vectorization\n",
    "\n",
    "# Trial and error exploration:\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df = 0.01, stop_words='english')\n",
    "# max_df=0.5 means words must not appear in more than half of docs\n",
    "\n",
    "# add 2- and 3-grams\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df = 0.01, stop_words='english', ngram_range=(1, 3))\n",
    "\n",
    "#Take min_df out again\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english', ngram_range=(1, 3))\n",
    "\n",
    "#Put min_df in again (number of parameters exploded!)\n",
    "\n",
    "# Interim chosen model before gridsearch:\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df = 0.75, min_df = 0.005, stop_words='english', ngram_range=(1, 3))\n",
    "\n",
    "features_train_transf = vectorizer.fit_transform(docs_train)\n",
    "features_test_transf  = vectorizer.transform(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check number of features\n",
    "all_feature_names = vectorizer.get_feature_names()\n",
    "print('Initial number of features after vectorisation:', len(all_feature_names))  \n",
    "\n",
    "# Note features will be further reduced with SelectPercentile below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select top x% of most individually useful features\n",
    "selector = SelectPercentile(f_classif, percentile=50) \n",
    "selector.fit(features_train_transf, labels_train)\n",
    "\n",
    "my_names = np.asarray(vectorizer.get_feature_names())[selector.get_support()]  #  Feature names (alphabetical)\n",
    "print('New number of features after SelectPercentile:', len(my_names)) # Halved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put into numpy arrays\n",
    "cut_features_train_transf = selector.transform(features_train_transf).toarray()\n",
    "cut_features_test_transf  = selector.transform(features_test_transf).toarray()\n",
    "\n",
    "# Convert labels from lists to numpy arrays\n",
    "labels_train = array(labels_train)\n",
    "labels_test = array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Number of training documents:')\n",
    "print(len(cut_features_train_transf))\n",
    "print(len(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(labels_train, return_counts=True)\n",
    "train_counts = dict(zip(unique, counts))\n",
    "print('Size of training dataset (documents):', train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now ready for scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "\n",
    "classifier = MultinomialNB() # Using default alpha for now\n",
    "fitted    = classifier.fit(cut_features_train_transf, labels_train)\n",
    "test_predicted = classifier.predict(cut_features_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple accuracy score\n",
    "print('Score:', classifier.score(cut_features_test_transf, labels_test))\n",
    "\n",
    "# Deeper metrics\n",
    "print(classification_report(test_predicted, labels_test, labels = parties))\n",
    "\n",
    "# Note that although all classes are represented in the test data, this report will throw a warning if the model never predicts\n",
    "# some of the classes. Here it often makes no predictions that a release comes from the Maori party (under-represented in data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: how many of the ones predicted to be (party) were in fact authored by that party?\n",
    "\n",
    "Recall: what proportion of releases authored by (party) were in fact correctly identified as such?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(labels_test, return_counts=True)\n",
    "test_counts = dict(zip(unique, counts))\n",
    "print('Make-up of the test set documents:', test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess which features (words) matter for each party - check for 'cheats'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make list of features with highest coefficient values, per class, from most to least important\n",
    "\n",
    "def list_top_features(classifier, feature_names, num_feat):\n",
    "    party_words = {}\n",
    "    counter = 0\n",
    "    top = 'top' + str(num_feat)\n",
    "    for i, label_train in enumerate(parties):          # enumerate loops with an automatic counter (in this case, i)\n",
    "        top = np.argsort(classifier.coef_[i])[::-1][0:num_feat]\n",
    "        list_top = str(', '.join(feature_names[j] for j in top)).split(',')\n",
    "        print(\" \")\n",
    "        print(parties[counter], 'most distinguishing words from most to ' + str(num_feat) + ':')\n",
    "        print(list_top)\n",
    "        party_words[parties[counter]] = list_top  # dict of lists where keys are parties\n",
    "        counter += 1        \n",
    "    return(party_words)\n",
    "\n",
    "\n",
    "# This version also prints out coefficients\n",
    "\n",
    "def list_top_features_with_coefs(classifier, feature_names, num_feat):\n",
    "    len_feature_names = len(feature_names)\n",
    "    for i in range(6):\n",
    "        print('\\n', parties[i], '\\n')  \n",
    "        diff = classifier.feature_log_prob_[i,:] - np.max(classifier.feature_log_prob_[-i:]) \n",
    "        name_diff = {}   \n",
    "        for j in range(len_feature_names):\n",
    "            name_diff[feature_names[j]] = diff[j]\n",
    "            names_diff_sorted = sorted(name_diff.items(), key = op.itemgetter(1), reverse = True)\n",
    "            \n",
    "        for k in range(num_feat):\n",
    "            print(k, names_diff_sorted[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_names = np.asarray(vectorizer.get_feature_names())[selector.get_support()]    \n",
    "\n",
    "party_words = list_top_features(classifier, my_names, 100) \n",
    "\n",
    "list_top_features_with_coefs(classifier, my_names, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So which parties does it mix up? eg left (Labour, Greens, Maori) vs right (National, ACT)? Or is it more random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix')\n",
    "print(metrics.confusion_matrix(labels_test, test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Confusion Matrix\n",
    "\n",
    "### Example run of my interim chosen model\n",
    "Confusion Matrix\n",
    "[[ 6  0  2  0  3 15]\n",
    " [ 0  2  3  0  7 10]\n",
    " [ 0  0 13  0 10 11]\n",
    " [ 0  0  1  0  6  3]\n",
    " [ 0  0  0  0 44  1]\n",
    " [ 0  0  0  0  1 46]]\n",
    "\n",
    "\n",
    "Test sample counts: {'act': 26, 'green': 22, 'labour': 34, 'maori': 10, 'national': 45, 'nzfirst': 47}\n",
    "\n",
    "The model has a strong tendency to overpredict the classes with the most examples, namely National and NZ First (the last two columns). It totally failed in identifying Maori party releases and was nearly as poor for the Green party.\n",
    " \n",
    "https://stackoverflow.com/questions/30746460/how-to-interpret-scikits-learn-confusion-matrix-and-classification-report\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interim model before optimisation\n",
    "\n",
    "\n",
    "Score: 0.614130434783\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        act       0.27      1.00      0.42         7\n",
    "      green       0.05      1.00      0.09         1\n",
    "     labour       0.53      0.67      0.59        27\n",
    "      maori       0.00      0.00      0.00         0\n",
    "   national       0.96      0.63      0.76        68\n",
    "    nzfirst       0.94      0.54      0.69        81\n",
    "\n",
    "avg / total       0.85      0.61      0.69       184\n",
    "\n",
    "\n",
    "\n",
    "### Total fail for the Maori and pretty bad for Green parties when maximising score (accuracy). Try Gridsearch using equally weighted F1 score instead and hopefully improve the recall for the smaller parties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a pipeline to make Gridsearching different models easier\n",
    "\n",
    "text_clf = Pipeline([('vect', TfidfVectorizer(sublinear_tf=True, stop_words='english', ngram_range=(1, 3), max_df=0.9, min_df=0.005)),\n",
    "                     ('selector', SelectPercentile(percentile = 60)),\n",
    "                     ('clf', MultinomialNB()), \n",
    "    # Put optimised min_df & max_df into vectorizer and optimised percentile into SelectPercentile for fine-tuning round\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initial run:\n",
    "\"\"\"\n",
    "parameters = {'vect__max_df': (0.5, 0.7, 0.9),\n",
    "              'vect__min_df': (0.005, 0.01),\n",
    "              'selector__percentile': (40, 60, 80),\n",
    "              'clf__alpha': (0.0001, 0.001, 0.01) }  # Laplace smoothing\n",
    "\n",
    "\"\"\"\n",
    "# Later run: fine-tune:\n",
    "parameters = {'clf__alpha': (0.005, 0.01, 0.05, 0.1, 0.2) }  # Laplace smoothing\n",
    "\n",
    "\n",
    "# Scoring:\n",
    "scoring = {'F1':       make_scorer(f1_score, average='weighted'), \n",
    "           'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "# average='weighted': Calculate metrics for each label, and find their average, weighted by support \n",
    "# (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; \n",
    "# it can result in an F-score that is not between precision and recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, scoring = scoring, refit = 'F1', cv = 6)\n",
    "gs_clf.fit(docs_train, labels_train)\n",
    "results = gs_clf.cv_results_\n",
    "\n",
    "# Setting refit='F1', refits an estimator on the whole dataset with the\n",
    "# parameter setting that has the best F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Best score\")\n",
    "print(gs_clf.best_score_)   \n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best identified parameters: \n",
    "clf__alpha: 0.01\n",
    "selector__percentile: 60\n",
    "vect__max_df: 0.9\n",
    "vect__min_df: 0.005\n",
    "\n",
    "Score with these parameters: 0.8027\n",
    "NOTE: On one run it chose max_df = 0.5 with a similar score. It doesn't seem to matter too much really\n",
    "\n",
    "Now fine-tune alpha, putting the optimised values for the other parameters as above into the Pipeline.\n",
    "\n",
    "FINE-TUNED BEST MODEL: actually it just chose alpha = 0.01 again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now test how a model fitted with these parameters does on held out test set\n",
    "# Need to re-run from vectorizer stage with these parameters\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df = 0.9, min_df = 0.005, stop_words='english', ngram_range=(1, 3))\n",
    "features_train_transf = vectorizer.fit_transform(docs_train)\n",
    "features_test_transf  = vectorizer.transform(docs_test)\n",
    "selector = SelectPercentile(f_classif, percentile=60) \n",
    "selector.fit(features_train_transf, labels_train)\n",
    "cut_features_train_transf = selector.transform(features_train_transf).toarray()\n",
    "cut_features_test_transf  = selector.transform(features_test_transf).toarray()\n",
    "labels_train = array(labels_train)\n",
    "labels_test = array(labels_test)\n",
    "classifier = MultinomialNB(alpha = 0.01)\n",
    "fitted    = classifier.fit(cut_features_train_transf, labels_train)\n",
    "test_predicted = classifier.predict(cut_features_test_transf)\n",
    "\n",
    "print(classification_report(test_predicted, labels_test, labels=parties))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of optimised model on held-out test set:\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "        act       0.81      0.78      0.79        27\n",
    "      green       0.45      0.59      0.51        17\n",
    "     labour       0.68      0.82      0.74        28\n",
    "      maori       0.90      0.90      0.90        10\n",
    "   national       0.91      0.82      0.86        50\n",
    "    nzfirst       0.87      0.79      0.83        52\n",
    "\n",
    "avg / total       0.81      0.79      0.79       184\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now to test out optimised model's robustness more thoroughly, go to Vet_optimised_model notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
