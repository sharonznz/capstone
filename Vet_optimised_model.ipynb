{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimised model did extremely well on the held-out test set.\n",
    "Now subject it to other robustness checks after running the vectorizer again with the optimised values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import operator as op\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parties = ['act', 'green', 'labour', 'maori', 'national', 'nzfirst']\n",
    "\n",
    "strip_list = ['Posted by', '\\n', 'Jacinda', 'Ardern', 'Steven', 'Joyce', ' Bill ', 'English', 'Carmel', 'Sepuloni', 'Barry',\n",
    "                '2017', 'David', 'Clark', 'Phil', 'Twyford', 'Michael', 'Wood', 'Chris ', 'Hipkins', 'Grant', 'Maggie',\n",
    "                'Robertson', 'Greg', 'O’Connor', 'Andrew', ' Little', 'Winston', 'Peters', 'Damien', 'O\\'Connor',\n",
    "                'Kelvin', 'Davis', 'Phil', 'Twyford', 'Megan', 'Woods', 'Parker', 'Nanaia', 'Mahuta', 'Paula', 'Bennett',\n",
    "                'Carter', 'Gerry', 'Brownlee', 'Simon', ' Bridges', ' Amy', 'Adams', 'Jonathan', 'Coleman', 'Christopher',\n",
    "                'Finlayson', 'Woodhouse', 'Nathan', 'Guy', 'Anne', 'Tolley', ' Ron ', 'Mark', 'Marama', 'Fox', ' Te ', \n",
    "                'Ururoa', 'Flavel', 'Jones', 'Shane', 'Taurima', 'Seymour', 'James', 'Shaw', 'Marama', 'Davidson', ' Dr ',\n",
    "                'Julie', 'Anne', 'Genter', 'Jan ', 'Logie', 'Eugenie', 'Sage', 'Gareth', 'Hughes', 'Steffan', 'Browning',\n",
    "                'Rt', 'Hon', 'Nick', 'Smith', 'Nikki', 'Kaye', 'Nicky', 'Wagner', 'Minister', 'Paul', 'Goldsmith',\n",
    "                'ACT', ' National ', 'Green Party', 'Labour', 'First ', 'ENDS', '.', ',' '\\\"', '\\'', 'Māori Party',\n",
    "                '“','”', 'Facebook5Twitter', 'Steffan', 'Browning', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                'Carlton', 'Burke', 'Chadwick', 'Catherine', 'Christine', 'Alex', 'Alexander', 'Baker', 'Carolyn', \n",
    "                'Alyssa', 'Brown', 'Bob', 'Byrn', 'Augustine', 'Crawford', 'Antonio', 'Claudetta', 'Christina', 'Collins'\n",
    "                'Buckner', 'centre', 'Ben', 'Boyden',  'Alan', 'Bosley', '’', 'Alastair', 'Ballantyne', 'Bruno', 'Cecelia',\n",
    "                 'Allan', 'Bernard', 'Anderson', 'Andrea', 'Tim', 'spokesperson', 'Scott', 'Simpson', 'Epsom', 'Metiria',\n",
    "                 'Turei', 'said', 'say', 'John Key', 'John', 'Tukoroirangi Morgan', 'Dame', 'Mei', 'Reedy', 'Leader', \n",
    "                 'Northland', 'Member of Parliament', 'Spokesperson', 'Don', 'Houlbrook', 'Stephen', 'Todd', 'Barclay',\n",
    "                 'Morgan', 'Tariana', 'Turia', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday',\n",
    "                 'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', \n",
    "                 'November', 'December', 'Southland', 'Judith', 'Collins', 'Jacqui', 'Dean', 'Bhupind', 'Singh', 'van Velden',\n",
    "                 'govt.nz', 'Mitchel', 'New Zealand', 'First', 'NZ', 'Mitchell', 'Tracy', 'Martin', 'Mike', 'MP', 'Prosser',\n",
    "                 'William', 'Sio', 'Don', 'Zealand', 'Aupito', 'Kevin', 'Hague', 'Bhupind', 'Singh', 'Louise', 'Upston'] \n",
    "\n",
    "strip_from_stemmed = ['conclusionth', 'bennet', 'brydon', 'bosley', 'centrewellington', 'countrynew', 'ballantyn', 'allan',\n",
    "                     'delahunti', ',', 'beth', 'ms', 'mr', 'www', 'http', 'media contact', 'govt', 'nz'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in(party, folder):\n",
    "    release_text_list = []\n",
    "    for filename in os.listdir(folder):\n",
    "        full_path = os.path.join(party, filename)\n",
    "        #print(full_path)\n",
    "        file_obj = open(os.path.join(party, filename), 'r', encoding='utf8')\n",
    "        content = file_obj.read()\n",
    "        file_obj.close()\n",
    "        release_text_list.append(content)\n",
    "    return(release_text_list)\n",
    "# Create dictionary where key = label (party) and value = list of release text strings.\n",
    "\n",
    "dict_of_text_lists = {}\n",
    "list_of_all_texts = []\n",
    "list_of_all_party_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    list_of_all_party_texts = read_in(party, party)\n",
    "    dict_of_text_lists[party] = list_of_all_party_texts\n",
    "    print('Total docs for party', party, '=', len(list_of_all_party_texts))\n",
    "    list_of_all_texts = list_of_all_texts + list_of_all_party_texts\n",
    "    list_of_all_party_texts = []\n",
    "\n",
    "print('Total documents: ', len(list_of_all_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary where key = label (party) and value = list of release text strings.\n",
    "\n",
    "dict_of_text_lists = {}\n",
    "list_of_all_texts = []\n",
    "list_of_all_party_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    list_of_all_party_texts = read_in(party, party)\n",
    "    dict_of_text_lists[party] = list_of_all_party_texts\n",
    "    print('Total docs for party', party, '=', len(list_of_all_party_texts))\n",
    "    list_of_all_texts = list_of_all_texts + list_of_all_party_texts\n",
    "    list_of_all_party_texts = []\n",
    "\n",
    "print('Total documents: ', len(list_of_all_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample National to improve dataset balance\n",
    "# If re-running, need to run the cell above first!\n",
    "\n",
    "print ('Removing three out of every four National Party press release over time:')\n",
    "print('National had', len(dict_of_text_lists['national']), 'press releases')\n",
    "top_end = len(dict_of_text_lists['national'])\n",
    "              \n",
    "for i in range(0, int(top_end/4)):\n",
    "#for i in range(1, int(top_end/4)):  # Robustness check - try a different quarter\n",
    "#for i in range(2, int(top_end/4)):  # Robustness check - try a different quarter\n",
    "#for i in range(3, int(top_end/4)):  # Robustness check - try a different quarter\n",
    "    del dict_of_text_lists['national'][i]\n",
    "    del dict_of_text_lists['national'][i]        \n",
    "    del dict_of_text_lists['national'][i]        \n",
    "    i += 3\n",
    "    \n",
    "print('National now has', len(dict_of_text_lists['national']), 'press releases')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of names (giveaways for the authoring problem) and a few other problematic ('cheat') strings, and stem the text\n",
    "\n",
    "stem_words = []\n",
    "party_list_of_proc_texts = []\n",
    "dict_of_proc_text_lists = {}\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "for party in parties:\n",
    "    party_list_of_proc_texts = []\n",
    "    \n",
    "    # Remove the words in my manual strip list above\n",
    "    for text in dict_of_text_lists[party]:\n",
    "        strip_text = text\n",
    "        for goner in strip_list:\n",
    "            if goner in text:\n",
    "                strip_text = strip_text.replace(goner, ' ')                \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # Apply Snowball stemmer:\n",
    "        stem_words = []\n",
    "        text_words = strip_text.split()\n",
    "        for word in text_words:\n",
    "            stem_word = stemmer.stem(word)\n",
    "            stem_words.append(stem_word)\n",
    "        stem_text = \" \".join(stem_words)\n",
    "        \n",
    "        # Strip a few problematic strings from the stemmed text:\n",
    "        for stemword in strip_from_stemmed:\n",
    "            if stemword in stem_text:\n",
    "                stem_text = stem_text.replace(stemword, ' ')\n",
    "        party_list_of_proc_texts.append(stem_text)\n",
    "        \n",
    "    # Put list of processed party texts in dictionary with party as key\n",
    "    dict_of_proc_text_lists[party] = party_list_of_proc_texts\n",
    "\n",
    "        \n",
    "    \n",
    "print('Example:')    \n",
    "print('Original text:', text[:80])\n",
    "print('\\n', 'Stripped text:', strip_text[:80])\n",
    "print('\\n', 'After stem:' , stem_text[:80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (unlabelled) processed texts\n",
    "list_of_all_proc_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    procd = dict_of_proc_text_lists[party]\n",
    "    list_of_all_proc_texts = list_of_all_proc_texts + procd\n",
    "    \n",
    "print(len(list_of_all_proc_texts))\n",
    "    \n",
    "# Make a list of party authors that will match up with the texts \n",
    "party_match = []\n",
    "for party in dict_of_proc_text_lists:\n",
    "    for text in dict_of_proc_text_lists[party]:\n",
    "        party_match.append(party)\n",
    "        \n",
    "print(len(party_match))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets:\n",
    "docs_train, docs_test, labels_train, labels_test = model_selection.train_test_split(list_of_all_proc_texts, \n",
    "                                                                                    party_match, \n",
    "                                                                                    test_size = 0.2,\n",
    "                                                                                    stratify = party_match)\n",
    "# Stratify ensures same balance of test as training data\n",
    "print(len(docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.9, min_df = 0.005, stop_words='english', ngram_range=(1, 3))\n",
    "features_train_transf = top_vectorizer.fit_transform(docs_train)\n",
    "features_test_transf  = top_vectorizer.transform(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_names = top_vectorizer.get_feature_names()\n",
    "print('Initial number of features after vectorisation:', len(all_feature_names))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top x% of most individually useful features\n",
    "top_selector = SelectPercentile(f_classif, percentile=60) \n",
    "top_selector.fit(features_train_transf, labels_train)\n",
    "\n",
    "my_names = np.asarray(top_vectorizer.get_feature_names())[top_selector.get_support()]  #  Feature names (alphabetical)\n",
    "print('New number of features after SelectPercentile:', len(my_names)) # Halved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and convert to numpy arrays\n",
    "cut_features_train_transf = top_selector.transform(features_train_transf).toarray()\n",
    "cut_features_test_transf  = top_selector.transform(features_test_transf).toarray()\n",
    "\n",
    "# Convert labels from lists to numpy arrays\n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now ready for Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nb = MultinomialNB(alpha = 0.01)\n",
    "top_fitted     = top_nb.fit(cut_features_train_transf, labels_train)\n",
    "test_predicted = top_nb.predict(cut_features_test_transf) \n",
    "print(classification_report(test_predicted, labels_test, labels = parties))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "        act       0.77      0.91      0.83        22\n",
    "      green       0.64      0.78      0.70        18\n",
    "     labour       0.85      0.72      0.78        40\n",
    "      maori       0.80      0.89      0.84         9\n",
    "   national       0.89      0.89      0.89        45\n",
    "    nzfirst       0.85      0.80      0.82        50\n",
    "\n",
    "avg / total       0.83      0.82      0.82       184\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix')\n",
    "print(metrics.confusion_matrix(labels_test, test_predicted))\n",
    "# {'act': 26, 'green': 22, 'labour': 34, 'maori': 10, 'national': 60, 'nzfirst': 47}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Confusion Matrix\n",
    "[[20  1  1  0  1  3]\n",
    " [ 0 14  3  0  2  3]\n",
    " [ 1  0 29  1  0  3]\n",
    " [ 0  1  0  8  1  0]\n",
    " [ 0  1  3  0 40  1]\n",
    " [ 1  1  4  0  1 40]]\n",
    "In [ ]:\n",
    "\n",
    "\n",
    " \n",
    " Looks much better. Much less tendency to always predict National or NZ First. Maori party accuracy is excellent!\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Robustness check 1: try different quarters of National data\n",
    "\n",
    "# Result with alternative 1:\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "        act       0.81      0.81      0.81        26\n",
    "      green       0.55      0.86      0.67        14\n",
    "     labour       0.76      0.76      0.76        34\n",
    "      maori       0.80      0.89      0.84         9\n",
    "   national       0.98      0.83      0.90        53\n",
    "    nzfirst       0.74      0.73      0.74        48\n",
    "\n",
    "avg / total       0.81      0.79      0.80       184\n",
    "\n",
    "# Result with alternative 2:\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "        act       0.88      0.96      0.92        24\n",
    "      green       0.55      0.86      0.67        14\n",
    "     labour       0.62      0.88      0.72        24\n",
    "      maori       0.80      1.00      0.89         8\n",
    "   national       0.96      0.85      0.90        52\n",
    "    nzfirst       1.00      0.75      0.85        63\n",
    "\n",
    "avg / total       0.88      0.84      0.85       185\n",
    "\n",
    "\n",
    "# Result with alternative 3:\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "        act       0.81      0.88      0.84        24\n",
    "      green       0.55      0.75      0.63        16\n",
    "     labour       0.74      0.78      0.76        32\n",
    "      maori       0.80      1.00      0.89         8\n",
    "   national       0.96      0.83      0.89        54\n",
    "    nzfirst       0.87      0.79      0.83        52\n",
    "\n",
    "avg / total       0.83      0.82      0.82       186\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make list of features with highest coefficient values, per class, from most to least important\n",
    "\n",
    "def list_top_features(classifier, feature_names, num_feat):\n",
    "    party_words = {}\n",
    "    counter = 0\n",
    "    top = 'top' + str(num_feat)\n",
    "    for i, label_train in enumerate(parties):          # enumerate loops with an automatic counter (in this case, i)\n",
    "        top = np.argsort(classifier.coef_[i])[::-1][0:num_feat]\n",
    "        list_top = str(', '.join(feature_names[j] for j in top)).split(',')\n",
    "        print(\" \")\n",
    "        print(parties[counter], 'most distinguishing words from most to ' + str(num_feat) + ':')\n",
    "        print(list_top)\n",
    "        party_words[parties[counter]] = list_top  # dict of lists where keys are parties\n",
    "        counter += 1        \n",
    "    return(party_words)\n",
    "\n",
    "\n",
    "\n",
    "# This version also prints out coefficients:\n",
    "\n",
    "def list_top_features_with_coefs(classifier, feature_names, num_feat):\n",
    "    word_nb_coef = {}\n",
    "    party_word_nb_coef = {}\n",
    "    maximum = 0\n",
    "    minimum = 0\n",
    "    \n",
    "    len_feature_names = len(feature_names)\n",
    "    for i in range(6):\n",
    "        print('\\n', parties[i], '\\n')  \n",
    "        diff = classifier.feature_log_prob_[i,:] - np.max(classifier.feature_log_prob_[-i:]) # Only works for NB\n",
    "        \n",
    "        name_diff = {}   \n",
    "        for j in range(len_feature_names):\n",
    "            name_diff[feature_names[j]] = diff[j]\n",
    "            names_diff_sorted = sorted(name_diff.items(), key = op.itemgetter(1), reverse = True)\n",
    "        # Check min coef - any negative?\n",
    "            if diff[j] < minimum:\n",
    "                minimum = diff[j]\n",
    "            else:\n",
    "                pass\n",
    "            if diff[j] > maximum:\n",
    "                maximum = diff[j]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for k in range(num_feat):\n",
    "            print(k, names_diff_sorted[k])\n",
    "            word_nb_coef[names_diff_sorted[k][0]] = names_diff_sorted[k][1] # Dictionary of word coefficients            \n",
    "        party_word_nb_coef[parties[i]] = word_nb_coef    \n",
    "        print ('maximum', maximum, 'minimum', minimum)\n",
    "    return party_word_nb_coef       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nb_party_words = list_top_features(top_nb, my_names, 200)   \n",
    "party_word_nb_coef = list_top_features_with_coefs(top_nb, my_names, 200) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (party_word_nb_coef['green']['river']) # test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a linear SVC for comparison - use the above values for feature selection (min_df and max_df in the Tfidf vectorizer, \n",
    "# percentile in the selector) and only gridsearch the SVC hyperparameter C.\n",
    "\n",
    "text_svc_clf = Pipeline([('vect', TfidfVectorizer(sublinear_tf=True, max_df = 0.9, min_df = 0.005, \n",
    "                                                  stop_words='english', ngram_range=(1, 3))),\n",
    "                         ('selector', SelectPercentile(percentile = 60)),\n",
    "                         ('svc_clf', LinearSVC(class_weight = 'balanced'))])\n",
    "text_svc_clf.fit(docs_train, labels_train) \n",
    "\n",
    "# class_weight: Set the parameter C of class i to class_weight[i]*C for SVC.\n",
    "# If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y \n",
    "# to automatically adjust weights inversely proportional to class frequencies in the input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch LinearSVC over C\n",
    "svc_parameters = {'svc_clf__C': (0.001, 0.01, 0.1, 1, 10, 100, 1000)}\n",
    "\n",
    "svc_scoring = {'F1':       make_scorer(f1_score, average='weighted'), \n",
    "               'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "gs_svc_clf = GridSearchCV(text_svc_clf, svc_parameters, scoring = svc_scoring, refit = 'F1', cv = 6)\n",
    "gs_svc_clf.fit(docs_train, labels_train)\n",
    "svc_results = gs_svc_clf.cv_results_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best score\")\n",
    "print(gs_svc_clf.best_score_)   \n",
    "\n",
    "for param_name in sorted(svc_parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_svc_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use on test set\n",
    "svc_test_predicted = gs_svc_clf.predict(docs_test)\n",
    "\n",
    "# Assess performance:\n",
    "print(classification_report(svc_test_predicted, labels_test, labels = parties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It beat the Naive Bayes\n",
    "\n",
    "SVC:\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "        act       0.77      0.95      0.85        21\n",
    "      green       0.73      0.84      0.78        19\n",
    "     labour       0.85      0.88      0.87        33\n",
    "      maori       0.80      1.00      0.89         8\n",
    "   national       0.93      0.89      0.91        47\n",
    "    nzfirst       0.91      0.77      0.83        56\n",
    "\n",
    "avg / total       0.87      0.86      0.86       184\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vs best NB with same features and test set:\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "        act       0.81      0.78      0.79        27\n",
    "      green       0.45      0.59      0.51        17\n",
    "     labour       0.68      0.82      0.74        28\n",
    "      maori       0.90      0.90      0.90        10\n",
    "   national       0.91      0.82      0.86        50\n",
    "    nzfirst       0.87      0.79      0.83        52\n",
    "\n",
    "avg / total       0.81      0.79      0.79       184\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine top features and see if there is much crossover \n",
    "top_svc = LinearSVC(class_weight = 'balanced', C = 1)\n",
    "top_svc.fit(cut_features_train_transf, labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_top_svc_features_with_coefs(classifier, feature_names, num_feat):\n",
    "    len_feature_names = len(feature_names)\n",
    "    word_nb_coef = {}\n",
    "    minimum = 0\n",
    "    maximum = 0\n",
    "    for i in range(6):\n",
    "        print('\\n', parties[i], '\\n')  \n",
    "        coef = classifier.coef_[i,:]  \n",
    "        name_coef = {}   \n",
    "        for j in range(len_feature_names):\n",
    "            name_coef[feature_names[j]] = coef[j]\n",
    "            names_coef_sorted = sorted(name_coef.items(), key = op.itemgetter(1), reverse = True)\n",
    "        # Check min coef - any negative?\n",
    "            if classifier.coef_[i,j] > minimum:\n",
    "                minimum = classifier.coef_[i,j]\n",
    "            else:\n",
    "                pass\n",
    "            if classifier.coef_[i,j] < maximum:\n",
    "                maximum = classifier.coef_[i,j]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for k in range(num_feat):\n",
    "            print(k, names_coef_sorted[k])   \n",
    "        print(\"max:\", maximum, \"min\", minimum)\n",
    "        \n",
    " # SVC coefficients can go negative - squaring is a bad idea.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_svc_party_words = list_top_features(top_svc, my_names, 200)   \n",
    "list_top_svc_features_with_coefs(top_svc, my_names, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nb_party_words = list_top_features(top_nb, my_names, 200)   \n",
    "#print(type(top_nb_party_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with top nb:\n",
    "\n",
    "# Calculate crossover between top Naive Bayes model and top SVC model:\n",
    "common_words = {}\n",
    "for party in parties:\n",
    "    if len(top_nb_party_words[party]) == len(top_svc_party_words[party]):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Warning: invalid comparison - the lists are different lengths.\")\n",
    "    common = list(set(top_nb_party_words[party]).intersection(top_svc_party_words[party]))  \n",
    "    print(party, \":\", len(common), \"out of\", len(top_nb_party_words[party]), \"words are the same. (\", \n",
    "          100*len(common)/len(top_svc_party_words[party]), '%)')\n",
    "    common_words[party] = common\n",
    "    print(common)\n",
    "    \n",
    "# I'm more comfortable interpreting the Naive Bayes coefficients so I will use them, but use the cross-over list.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "act : 90 out of 200 words are the same. ( 45.0 %)\n",
    "[' partnership school', ' manag act', ' choic', ' right', ' person', ' instead', ' campaign', ' homebuild', ' left', ' hous shortag', ' poll', ' onli', ' partnership', ' reveal', ' point', ' ll', ' red', ' share', ' polici', ' age', ' freedom', ' salari', ' resourc manag', ' achiev', ' cut tax', ' pay tax', ' busi', ' far', ' entir', ' life', ' current', ' properti', ' debat', ' tax', ' wait', ' simpli', ' benefit', ' turn', ' lie', ' replac', ' choos', ' voter', ' rural', ' market', ' polit', ' shortag', ' caus', ' scrap', ' confisc', ' power', ' infrastructur', ' use', ' anti', ' red tape', ' mean', ' welcom', ' resourc manag act', ' save', ' parti vote', ' believ', ' onli parti', ' politician', ' let', ' tape', ' cut', ' road price', ' book', ' stronger', ' green', ' taxpay money', ' actual', ' honest', ' strong', ' state school', ' superannu', ' right govern', ' words', ' candid', ' effect', ' alreadi', ' case', ' election', ' rais', ' assist die', ' hous market', ' pay', ' cap', ' kid', ' resign', ' confirm']\n",
    "green : 84 out of 200 words are the same. ( 42.0 %)\n",
    "[' foreign trust', ' real', ' risk', ' inquiri', ' people', ' today govern', ' aren', ' child poverti', ' concern', ' river', ' power', ' roch', ' includ', ' child', ' capit gain', ' investor', ' refus', ' need', ' landcorp', ' left', ' lead', ' farm', ' transport', ' control', ' agreement', ' larg', ' ensur', ' convers', ' law', ' select committe', ' cold', ' nativ', ' rivers', ' industri', ' protect', ' clear', ' properti specul', ' famili', ' work', ' govern need', ' ignor', ' chang', ' decis', ' percent', ' profit', ' encourag', ' better', ' benefit', ' marin', ' won', ' make', ' environ', ' bank', ' stop', ' forest', ' review', ' poverti', ' hous market', ' like', ' veri', ' children', ' thousand', ' govern stop', ' kind', ' human', ' choos', ' capit', ' push', ' low', ' kid', ' leav', ' cut', ' plan', ' amend', ' home', ' close', ' pollut', ' climat', ' isn', ' son', ' doesn', ' green', ' select', ' look']\n",
    "labour : 77 out of 200 words are the same. ( 38.5 %)\n",
    "[' sure', ' happen', ' cent', ' long', ' citi', ' modern', ' rapid', ' health', ' dhb', ' communiti', ' auckland', ' river', ' hospit', ' climat chang', ' pressur', ' addit', ' teacher', ' fail', ' cost', ' wrong', ' ask', ' deliv', ' healthi', ' ensur', ' allow', ' opposit', ' start', ' financ', ' fix', ' fair', ' vision', ' specul', ' commit', ' promis', ' motel', ' world class', ' build', ' time fresh approach', ' boost', ' simpli', ' skill', ' believ', ' alreadi', ' restor', ' fiscal plan', ' better', ' state', ' share', ' years', ' place', ' invest', ' fix hous', ' claim', ' evid', ' thing', ' time', ' approach', ' clean', ' afford', ' nation', ' question', ' kiwibuild', ' polici', ' fact', ' answer', ' tax cut', ' fresh approach', ' make sure', ' elect', ' qualiti', ' plan', ' time fresh', ' care', ' lack', ' underfund', ' deserv', ' state hous']\n",
    "maori : 118 out of 200 words are the same. ( 59.0 %)\n",
    "[' parti lead', ' issu', 'māori', ' papa', ' establish', ' presid', ' inspir', ' tribut', ' te', ' hapū', ' influenc', ' aspir', ' kai', ' tonight', ' mana', ' years', ' ko', ' togeth', ' approach', ' māori seat', ' vote', ' general roll', ' whānau hapū', ' tangata', ' care', ' issue', ' tourism', ' māori pasifika', ' suffer', ' life', ' don', ' year māori', ' ringa', ' atu', ' correct', ' dream', ' whakapapa', ' repres', ' budget', ' kōhanga reo', ' tamariki', ' network', ' tai', ' heard', ' host', ' kaupapa', ' make', ' lie', ' māori tourism', ' aotearoa', ' peopl', ' ethnic', ' voter', ' fight', ' abus', ' voic', ' parti', ' whānau', ' tonga', ' seat', ' māori hous', ' long', ' kura', ' taar', ' state care', ' stori', ' ora', ' iwi', ' prison', ' job', ' wellb', ' million budget', ' manurewa', ' kei', ' whānau hapū iwi', ' royal commiss', ' indigen', ' royal', ' non māori', ' seek', ' asian', ' veri', ' royal commiss inquiri', ' justic', ' especi', ' indigen peopl', ' think', ' seab', ' kōhanga', ' inquiri', ' speak', ' foreshor seab', ' lead', ' hand', ' reo', ' tabl', ' sharp', ' candid', ' seven', ' link', ' elector', ' ki', ' mokopuna', ' pasifika', ' term', ' hapū iwi', ' advoc', ' strength', ' foreshor', ' general', ' commiss inquiri', ' tai tonga', ' roll', ' mai', ' live', ' whānau ora', ' waitangi', ' commiss']\n",
    "national : 100 out of 200 words are the same. ( 50.0 %)\n",
    "[' servic', ' communiti', ' initi', ' visit', ' support', ' secur', ' complet', ' forc', ' deliv', ' improv', ' billion', ' trust', ' help', ' skill', ' chang', ' week', ' key', ' togeth', ' import', ' intern', ' month', ' process', ' role', ' achiev', ' busi', ' abl', ' technolog', ' cent', ' north', ' project', ' develop', ' current', ' progress', ' addit', ' agenc', ' design', ' continu', ' member', ' upston', ' aim', ' success', ' social hous', ' direct', ' upgrad', ' invest million', ' launch', ' today', ' inform', ' appoint', ' enabl', ' crown', ' digit', ' engag', ' encourag', ' extend', ' includ', ' provid', ' onlin', ' conserv', ' rang', ' contribut', ' signific', ' identifi', ' wider', ' follow', ' event', ' wellington', ' reduc', ' defenc', ' receiv', ' pacif', ' justic', ' play', ' increas', ' recoveri', ' connect', ' collabor', ' facil', ' south', ' regul', ' avail', ' meet', ' innov', ' partner', ' term', ' million', ' earthquak', ' implement', ' construct', ' group', ' practic', ' scienc', ' focus', ' social', ' bay', ' announc', ' number', ' today announc', ' announc today', ' associ']\n",
    "nzfirst : 73 out of 200 words are the same. ( 36.5 %)\n",
    "[' shut', ' hard', ' demand', ' noth', ' real', ' econom', ' year', ' mass', ' farmer', ' countri', ' scheme', ' auckland', ' concern', ' sort', ' deni', ' accept', ' farm', ' pm', ' know', ' super', ' hand', ' massiv', ' got', ' sale', ' fujixerox', ' south', ' talk', ' start', ' face', ' despit', ' industri', ' job', ' biosecur', ' ago', ' fraud', ' sold', ' net', ' taken', ' promis', ' mass immigr', ' national', ' hold', ' want', ' small', ' doe', ' parliament', ' gst', ' prime', ' end', ' did', ' anoth', ' given', ' line', ' old', ' ownership', ' parliament today', ' crime', ' deal', ' tri', ' question', ' district', ' total', ' offic', ' matter', ' asset', ' economi', ' lack', ' maori', ' big', ' export', ' deputi', ' immigr', ' built']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NB coefficients to suitable number for wordle.net\n",
    "#party_word_nb_coef = list_top_features_with_coefs(top_nb, my_names, 200) \n",
    "\n",
    "party_word_cloud_coef = {}\n",
    "\n",
    "for party in parties:\n",
    "    print('\\n', party, '\\n')\n",
    "    party_word_cloud_coef[party] = {}\n",
    "    for word in common_words[party]:\n",
    "        word = word.strip()\n",
    "        party_word_cloud_coef[party][word] = str(int(100*(party_word_nb_coef[party][word] + 3))) # Make them positive integers\n",
    "        my_string = word + ':' + party_word_cloud_coef[party][word] # Print in format ready to drop into wordle.net\n",
    "        print (my_string)\n",
    "\n",
    "# These are still stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My best guess at de-stemming (removing unintelligible and very common words)\n",
    "\n",
    "act \n",
    "\n",
    "partnership school:61\n",
    "choice:96\n",
    "right:90\n",
    "campaign:38\n",
    "homebuilding:55\n",
    "left:220\n",
    "housing shortage:78\n",
    "poll:121\n",
    "online:116\n",
    "partnership:77\n",
    "reveal:62\n",
    "point:107\n",
    "red:158\n",
    "share:169\n",
    "policy:118\n",
    "age:103\n",
    "freedom:93\n",
    "salary:85\n",
    "resource management act:67\n",
    "achieve:98\n",
    "cut tax:69\n",
    "pay tax:55\n",
    "business:87\n",
    "property:225\n",
    "debate:104\n",
    "tax:69\n",
    "simplify:200\n",
    "benefit:98\n",
    "lie:80\n",
    "replace:106\n",
    "choose:205\n",
    "voter:96\n",
    "rural:71\n",
    "market:57\n",
    "politics:86\n",
    "shortage:171\n",
    "cause:203\n",
    "scrap:97\n",
    "confiscate:69\n",
    "power:36\n",
    "infrastructure:40\n",
    "anti:57\n",
    "red tape:166\n",
    "welcome:51\n",
    "save:191\n",
    "party vote:73\n",
    "believe:32\n",
    "only party:56\n",
    "politician:85\n",
    "let:62\n",
    "tape:164\n",
    "cut:212\n",
    "roading price:74\n",
    "stronger:130\n",
    "green:38\n",
    "taxpayers money:79\n",
    "honest:66\n",
    "strong:71\n",
    "state school:74\n",
    "superannuation:85\n",
    "right to govern:72\n",
    "words:106\n",
    "candid:156\n",
    "effect:82\n",
    "election:81\n",
    "raise:77\n",
    "assisted dying:55\n",
    "housing market:210\n",
    "pay:76\n",
    "cap:61\n",
    "kids:179\n",
    "resign:133\n",
    "confirm:80\n",
    "\n",
    " green \n",
    "\n",
    "foreign trust:220\n",
    "risk:242\n",
    "inquiry:123\n",
    "people:196\n",
    "today's government:181\n",
    "arena:191\n",
    "child poverty:209\n",
    "concern:71\n",
    "river:178\n",
    "power:36\n",
    "children:51\n",
    "capital gains:189\n",
    "investor:186\n",
    "refuse:196\n",
    "need:117\n",
    "Landcorp:216\n",
    "lead:217\n",
    "farm:78\n",
    "transport:38\n",
    "control:55\n",
    "agreement:225\n",
    "large:194\n",
    "conversation:209\n",
    "law:55\n",
    "select committee:212\n",
    "cold:193\n",
    "native:191\n",
    "rivers:214\n",
    "industry:92\n",
    "protect:73\n",
    "clear:43\n",
    "property speculators:195\n",
    "family:32\n",
    "work:99\n",
    "government needs:214\n",
    "ignore:155\n",
    "change:61\n",
    "decision:90\n",
    "profit:205\n",
    "encourage:69\n",
    "better:84\n",
    "benefit:98\n",
    "marine:178\n",
    "environment:95\n",
    "bank:53\n",
    "stop:49\n",
    "forest:180\n",
    "review:243\n",
    "poverty:223\n",
    "housing market:210\n",
    "children:128\n",
    "government to stop:179\n",
    "kind:194\n",
    "human:190\n",
    "choose:205\n",
    "capital:196\n",
    "push:206\n",
    "low:247\n",
    "kids:179\n",
    "leave:191\n",
    "cut:212\n",
    "plan:52\n",
    "amend:182\n",
    "home:61\n",
    "pollution:271\n",
    "climate:148\n",
    "green:38\n",
    "\n",
    " labour \n",
    "\n",
    "city:45\n",
    "modern:157\n",
    "rapid:151\n",
    "health:61\n",
    "DHB:167\n",
    "community:29\n",
    "Auckland:126\n",
    "river:178\n",
    "hospital:168\n",
    "climate change:149\n",
    "pressure:207\n",
    "teachers:195\n",
    "fail:228\n",
    "cost:57\n",
    "wrong:148\n",
    "deliver:107\n",
    "healthy:176\n",
    "ensure:117\n",
    "allow:201\n",
    "opposition:237\n",
    "financial:161\n",
    "fix:194\n",
    "fair:196\n",
    "vision:165\n",
    "speculators:173\n",
    "commit:111\n",
    "promise:80\n",
    "motel:151\n",
    "world class:144\n",
    "build:53\n",
    "time for a fresh approach:176\n",
    "simplify:200\n",
    "skill:114\n",
    "believe:32\n",
    "restore:150\n",
    "fiscal plan:160\n",
    "better:84\n",
    "state:77\n",
    "share:169\n",
    "place:83\n",
    "invest:36\n",
    "fix housing:148\n",
    "claim:91\n",
    "evidence:158\n",
    "time:108\n",
    "clean:152\n",
    "affordability:248\n",
    "nation:125\n",
    "question:104\n",
    "Kiwibuild:178\n",
    "policy:118\n",
    "fact:43\n",
    "tax cut:174\n",
    "fresh approach:268\n",
    "make sure:161\n",
    "election:95\n",
    "quality:186\n",
    "plan:52\n",
    "fresh:182\n",
    "care:123\n",
    "lack:30\n",
    "underfunded:151\n",
    "deserve:199\n",
    "state housing:211\n",
    "\n",
    " maori \n",
    "\n",
    "issue:48\n",
    "māori:300\n",
    "papa:105\n",
    "establish:86\n",
    "inspired:64\n",
    "tribute:76\n",
    "te:207\n",
    "hapū:126\n",
    "influence:71\n",
    "aspire:93\n",
    "kai:52\n",
    "mana:85\n",
    "together:60\n",
    "approach:72\n",
    "māori seats:123\n",
    "vote:153\n",
    "general roll:58\n",
    "whānau hapū:96\n",
    "tangata:84\n",
    "care:123\n",
    "issues:65\n",
    "tourism:85\n",
    "māori pasifika:52\n",
    "suffer:64\n",
    "life:90\n",
    "ringa:54\n",
    "atu:90\n",
    "correct:77\n",
    "dream:66\n",
    "whakapapa:52\n",
    "repressed:76\n",
    "budget:87\n",
    "kōhanga reo:111\n",
    "tamariki:150\n",
    "network:52\n",
    "tai:154\n",
    "host:112\n",
    "kaupapa:113\n",
    "make:70\n",
    "lie:80\n",
    "māori tourism:60\n",
    "Aotearoa:137\n",
    "people:96\n",
    "ethnicity:54\n",
    "voter:96\n",
    "fight:112\n",
    "voice:134\n",
    "party:144\n",
    "whānau:267\n",
    "tonga:119\n",
    "seat:156\n",
    "māori housing:73\n",
    "long:50\n",
    "kura:62\n",
    "state care:80\n",
    "story:91\n",
    "ora:176\n",
    "iwi:164\n",
    "prison:88\n",
    "job:102\n",
    "wellbeing:73\n",
    "millions budget:66\n",
    "Manurewa:87\n",
    "whānau hapū iwi:96\n",
    "indigenous:138\n",
    "non-māori:82\n",
    "Asian:72\n",
    "royal commission of inquiry:140\n",
    "justice:52\n",
    "seabed:55\n",
    "kōhanga:115\n",
    "inquiry:123\n",
    "foreshore and seabed:61\n",
    "reo:174\n",
    "candid:156\n",
    "electorate:50\n",
    "mokopuna:80\n",
    "Pasifika:94\n",
    "hapū iwi:120\n",
    "advocate:94\n",
    "strength:50\n",
    "foreshore:76\n",
    "tai tonga:125\n",
    "roll:92\n",
    "whānau ora:167\n",
    "waitangi:91\n",
    "commission:96\n",
    "\n",
    " national \n",
    "\n",
    "service:56\n",
    "community:29\n",
    "initial:111\n",
    "support:47\n",
    "secure:89\n",
    "complete:96\n",
    "force:61\n",
    "deliver:107\n",
    "improve:139\n",
    "billion:42\n",
    "trust:98\n",
    "help:42\n",
    "skill:114\n",
    "change:61\n",
    "week:54\n",
    "key:93\n",
    "together:60\n",
    "important:65\n",
    "intern:59\n",
    "process:66\n",
    "role:77\n",
    "achieve:98\n",
    "business:87\n",
    "able:76\n",
    "technology:75\n",
    "project:162\n",
    "develop:32\n",
    "current:97\n",
    "progress:62\n",
    "addition:79\n",
    "agency:92\n",
    "design:66\n",
    "continue:48\n",
    "aim:69\n",
    "success:100\n",
    "social housing:64\n",
    "direct:58\n",
    "upgrade:61\n",
    "invest millions:52\n",
    "launch:71\n",
    "inform:135\n",
    "appoint:64\n",
    "enable:83\n",
    "crown:59\n",
    "digital:61\n",
    "engage:70\n",
    "encourage:69\n",
    "extend:59\n",
    "include:159\n",
    "provide:178\n",
    "online:48\n",
    "conservation:49\n",
    "contribution:119\n",
    "significant:133\n",
    "identify:50\n",
    "follow:91\n",
    "wellington:67\n",
    "defence:79\n",
    "Pacific:111\n",
    "justice:52\n",
    "increase:36\n",
    "recovery:50\n",
    "connect:124\n",
    "collaboration:51\n",
    "facility:62\n",
    "regulation:51\n",
    "available:120\n",
    "innovation:102\n",
    "partner:80\n",
    "earthquake:51\n",
    "implement:73\n",
    "construction:62\n",
    "practical:47\n",
    "science:52\n",
    "focus:104\n",
    "social:110\n",
    "announced today:84\n",
    "association:90\n",
    "\n",
    " nzfirst \n",
    "\n",
    "hard:36\n",
    "demand:39\n",
    "nothing:84\n",
    "real:72\n",
    "economy:105\n",
    "mass:52\n",
    "farmers:64\n",
    "country:117\n",
    "scheme:60\n",
    "Auckland:126\n",
    "concern:71\n",
    "denial:42\n",
    "accept:53\n",
    "farms:78\n",
    "super:69\n",
    "massive:70\n",
    "sale:52\n",
    "start:65\n",
    "despite:55\n",
    "industry:92\n",
    "jobs:102\n",
    "biosecurity:48\n",
    "fraud:64\n",
    "sold:33\n",
    "promise:80\n",
    "mass immigration:38\n",
    "national:39\n",
    "want:114\n",
    "parliament:106\n",
    "GST:46\n",
    "ownership:43\n",
    "crime:44\n",
    "deal:41\n",
    "try:75\n",
    "question:104\n",
    "office:99\n",
    "asset:33\n",
    "economy:106\n",
    "lack:30\n",
    "maori:36\n",
    "export:98\n",
    "immigration:126\n",
    "built:55\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# READ IN NEW (OUT-OF-SAMPLE) TEST DOCUMENTS: LATEST PRESS RELEASES\n",
    "# Create dictionary where key = label (party) and value = list of release text strings.\n",
    "def read_in_new(party, folder):\n",
    "    release_text_list = []\n",
    "    for filename in os.listdir(folder):\n",
    "        full_path = os.path.join(folder, filename)\n",
    "        #print(full_path)\n",
    "        file_obj = open(os.path.join(folder, filename), 'r', encoding='utf8')\n",
    "        content = file_obj.read()\n",
    "        file_obj.close()\n",
    "        release_text_list.append(content)\n",
    "    return(release_text_list)\n",
    "\n",
    "dict_of_new_text_lists = {}\n",
    "list_of_all_new_texts = []\n",
    "list_of_all_new_party_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    new_folder = str(os.path.join('test', party))\n",
    "    print(new_folder)\n",
    "    list_of_all_new_party_texts = read_in_new(party, new_folder)\n",
    "    #print(list_of_all_new_party_texts[0][:100])\n",
    "    dict_of_new_text_lists[party] = list_of_all_new_party_texts\n",
    "    print('Total new docs for party', party, '=', len(list_of_all_new_party_texts))\n",
    "    list_of_all_new_texts = list_of_all_new_texts + list_of_all_new_party_texts\n",
    "    list_of_all_new_party_texts = []\n",
    "\n",
    "print('Total documents: ', len(list_of_all_new_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip and stem\n",
    "\n",
    "new_stem_words = []\n",
    "party_list_of_new_proc_texts = []\n",
    "dict_of_new_proc_text_lists = {}\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "for party in parties:\n",
    "    party_list_of_new_proc_texts = []\n",
    "    \n",
    "    # Remove the words in my manual strip list above\n",
    "    for text in dict_of_new_text_lists[party]:\n",
    "        strip_text = text\n",
    "        for goner in strip_list:\n",
    "            if goner in text:\n",
    "                strip_text = strip_text.replace(goner, ' ')                \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # Apply Snowball stemmer:\n",
    "        new_stem_words = []\n",
    "        text_words = strip_text.split()\n",
    "        for word in text_words:\n",
    "            stem_word = stemmer.stem(word)\n",
    "            new_stem_words.append(stem_word)\n",
    "        stem_text = \" \".join(new_stem_words)\n",
    "        \n",
    "        # Strip a few problematic strings from the stemmed text:\n",
    "        for stemword in strip_from_stemmed:\n",
    "            if stemword in stem_text:\n",
    "                stem_text = stem_text.replace(stemword, ' ')\n",
    "        party_list_of_new_proc_texts.append(stem_text)\n",
    "        \n",
    "    # Put list of processed party texts in dictionary with party as key\n",
    "    dict_of_new_proc_text_lists[party] = party_list_of_new_proc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (dict_of_new_proc_text_lists['act'][0][0:100])\n",
    "print (dict_of_new_proc_text_lists['green'][9][0:100])\n",
    "print (dict_of_new_proc_text_lists['labour'][0][0:100])\n",
    "print (dict_of_new_proc_text_lists['maori'][0][0:100])\n",
    "print (dict_of_new_proc_text_lists['national'][0][0:100])\n",
    "print (dict_of_new_proc_text_lists['nzfirst'][0][0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (unlabelled) processed texts\n",
    "list_of_all_new_proc_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    procd = dict_of_new_proc_text_lists[party]\n",
    "    list_of_all_new_proc_texts = list_of_all_new_proc_texts + procd\n",
    "    \n",
    "print(len(list_of_all_new_proc_texts))\n",
    "    \n",
    "# Make a list of party authors that will match up with the texts \n",
    "new_party_match = []\n",
    "for party in dict_of_new_proc_text_lists:\n",
    "    for text in dict_of_new_proc_text_lists[party]:\n",
    "        new_party_match.append(party)\n",
    "        \n",
    "print(len(new_party_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test2_transf = top_vectorizer.transform(list_of_all_new_proc_texts)\n",
    "cut_features_test2_transf  = top_selector.transform(features_test2_transf).toarray()\n",
    "print(cut_features_test2_transf.shape[0]) # sparse numpy array. shape[0] = 184, number of docs. shape[1] = 6336, num of features\n",
    "print(cut_features_test2_transf.shape[1]) # sparse numpy array. shape[0] = 184, number of docs. shape[1] = 3801, num of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_new_test = np.array(new_party_match)\n",
    "print(len(labels_new_test))\n",
    "print(features_test2_transf.shape[0]) # sparse numpy array. shape[0] = 184, number of docs. shape[1] = num of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes \n",
    "test_new_predicted_nb = top_nb.predict(cut_features_test2_transf) \n",
    "print(classification_report(test_new_predicted_nb, labels_new_test, labels = parties))\n",
    "print('Confusion Matrix')\n",
    "print(metrics.confusion_matrix(labels_new_test, test_new_predicted_nb))\n",
    "\n",
    "# SVC\n",
    "test_new_predicted_svc = top_svc.predict(cut_features_test2_transf) \n",
    "print(classification_report(test_new_predicted_svc, labels_new_test, labels = parties))\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(metrics.confusion_matrix(labels_new_test, test_new_predicted_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Optimised Naive Bayes model:\n",
    "\n",
    "\n",
    "            precision    recall  f1-score   support\n",
    "\n",
    "        act       0.71      0.75      0.73        16\n",
    "      green       0.45      0.86      0.59        21\n",
    "     labour       0.62      0.53      0.57        34\n",
    "      maori       0.89      0.73      0.80        11\n",
    "   national       0.71      0.75      0.73        64\n",
    "    nzfirst       0.90      0.50      0.64        38\n",
    "\n",
    "avg / total       0.71      0.67      0.67       184\n",
    "\n",
    "Confusion Matrix\n",
    "[[12  0  0  0  3  2]\n",
    " [ 1 18  7  3  7  4]\n",
    " [ 0  2 18  0  5  4]\n",
    " [ 0  0  0  8  0  1]\n",
    " [ 2  1  9  0 48  8]\n",
    " [ 1  0  0  0  1 19]]\n",
    " \n",
    " \n",
    " Optimised SVC model:\n",
    " \n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        act       0.71      0.75      0.73        16\n",
    "      green       0.70      0.85      0.77        33\n",
    "     labour       0.69      0.74      0.71        27\n",
    "      maori       0.89      0.73      0.80        11\n",
    "   national       0.74      0.82      0.78        61\n",
    "    nzfirst       0.95      0.56      0.70        36\n",
    "\n",
    "avg / total       0.77      0.75      0.75       184\n",
    "\n",
    "Confusion Matrix\n",
    "[[12  0  0  0  4  1]\n",
    " [ 0 28  3  2  4  3]\n",
    " [ 0  3 20  1  2  3]\n",
    " [ 0  0  0  8  0  1]\n",
    " [ 4  2  4  0 50  8]\n",
    " [ 0  0  0  0  1 20]]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
