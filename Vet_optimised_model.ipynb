{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimised model did extremely well on the held-out test set.\n",
    "Now subject it to other robustness checks after running the vectorizer again with the optimised values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import operator as op\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parties = ['act', 'green', 'labour', 'maori', 'national', 'nzfirst']\n",
    "\n",
    "strip_list = ['Posted by', '\\n', 'Jacinda', 'Ardern', 'Steven', 'Joyce', ' Bill ', 'English', 'Carmel', 'Sepuloni', 'Barry',\n",
    "                '2017', 'David', 'Clark', 'Phil', 'Twyford', 'Michael', 'Wood', 'Chris ', 'Hipkins', 'Grant', 'Maggie',\n",
    "                'Robertson', 'Greg', 'O’Connor', 'Andrew', ' Little', 'Winston', 'Peters', 'Damien', 'O\\'Connor',\n",
    "                'Kelvin', 'Davis', 'Phil', 'Twyford', 'Megan', 'Woods', 'Parker', 'Nanaia', 'Mahuta', 'Paula', 'Bennett',\n",
    "                'Carter', 'Gerry', 'Brownlee', 'Simon', ' Bridges', ' Amy', 'Adams', 'Jonathan', 'Coleman', 'Christopher',\n",
    "                'Finlayson', 'Woodhouse', 'Nathan', 'Guy', 'Anne', 'Tolley', ' Ron ', 'Mark', 'Marama', 'Fox', ' Te ', \n",
    "                'Ururoa', 'Flavel', 'Jones', 'Shane', 'Taurima', 'Seymour', 'James', 'Shaw', 'Marama', 'Davidson', ' Dr ',\n",
    "                'Julie', 'Anne', 'Genter', 'Jan ', 'Logie', 'Eugenie', 'Sage', 'Gareth', 'Hughes', 'Steffan', 'Browning',\n",
    "                'Rt', 'Hon', 'Nick', 'Smith', 'Nikki', 'Kaye', 'Nicky', 'Wagner', 'Minister', 'Paul', 'Goldsmith',\n",
    "                'ACT', ' National ', 'Green Party', 'Labour', 'First ', 'ENDS', '.', ',' '\\\"', '\\'', 'Māori Party',\n",
    "                '“','”', 'Facebook5Twitter', 'Steffan', 'Browning', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                'Carlton', 'Burke', 'Chadwick', 'Catherine', 'Christine', 'Alex', 'Alexander', 'Baker', 'Carolyn', \n",
    "                'Alyssa', 'Brown', 'Bob', 'Byrn', 'Augustine', 'Crawford', 'Antonio', 'Claudetta', 'Christina', 'Collins'\n",
    "                'Buckner', 'centre', 'Ben', 'Boyden',  'Alan', 'Bosley', '’', 'Alastair', 'Ballantyne', 'Bruno', 'Cecelia',\n",
    "                 'Allan', 'Bernard', 'Anderson', 'Andrea', 'Tim', 'spokesperson', 'Scott', 'Simpson', 'Epsom', 'Metiria',\n",
    "                 'Turei', 'said', 'say', 'John Key', 'John', 'Tukoroirangi Morgan', 'Dame', 'Mei', 'Reedy', 'Leader', \n",
    "                 'Northland', 'Member of Parliament', 'Spokesperson', 'Don', 'Houlbrook', 'Stephen', 'Todd', 'Barclay',\n",
    "                 'Morgan', 'Tariana', 'Turia', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday',\n",
    "                 'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', \n",
    "                 'November', 'December', 'Southland', 'Judith', 'Collins', 'Jacqui', 'Dean', 'Bhupind', 'Singh', 'van Velden',\n",
    "                 'govt.nz', 'Mitchel', 'New Zealand', 'First', 'NZ', 'Mitchell', 'Tracy', 'Martin', 'Mike', 'MP', 'Prosser',\n",
    "                 'William', 'Sio', 'Don', 'Zealand', 'Aupito', 'Kevin', 'Hague', 'Bhupind', 'Singh', 'Louise', 'Upston'] \n",
    "\n",
    "strip_from_stemmed = ['conclusionth', 'bennet', 'brydon', 'bosley', 'centrewellington', 'countrynew', 'ballantyn', 'allan',\n",
    "                     'delahunti', ',', 'beth', 'ms', 'mr', 'www', 'http', 'media contact', 'govt', 'nz'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs for party act = 130\n",
      "Total docs for party green = 110\n",
      "Total docs for party labour = 169\n",
      "Total docs for party maori = 48\n",
      "Total docs for party national = 900\n",
      "Total docs for party nzfirst = 235\n",
      "Total documents:  1592\n"
     ]
    }
   ],
   "source": [
    "def read_in(party, folder):\n",
    "    release_text_list = []\n",
    "    for filename in os.listdir(folder):\n",
    "        full_path = os.path.join(party, filename)\n",
    "        #print(full_path)\n",
    "        file_obj = open(os.path.join(party, filename), 'r', encoding='utf8')\n",
    "        content = file_obj.read()\n",
    "        file_obj.close()\n",
    "        release_text_list.append(content)\n",
    "    return(release_text_list)\n",
    "# Create dictionary where key = label (party) and value = list of release text strings.\n",
    "\n",
    "dict_of_text_lists = {}\n",
    "list_of_all_texts = []\n",
    "list_of_all_party_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    list_of_all_party_texts = read_in(party, party)\n",
    "    dict_of_text_lists[party] = list_of_all_party_texts\n",
    "    print('Total docs for party', party, '=', len(list_of_all_party_texts))\n",
    "    list_of_all_texts = list_of_all_texts + list_of_all_party_texts\n",
    "    list_of_all_party_texts = []\n",
    "\n",
    "print('Total documents: ', len(list_of_all_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs for party act = 130\n",
      "Total docs for party green = 110\n",
      "Total docs for party labour = 169\n",
      "Total docs for party maori = 48\n",
      "Total docs for party national = 900\n",
      "Total docs for party nzfirst = 235\n",
      "Total documents:  1592\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary where key = label (party) and value = list of release text strings.\n",
    "\n",
    "dict_of_text_lists = {}\n",
    "list_of_all_texts = []\n",
    "list_of_all_party_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    list_of_all_party_texts = read_in(party, party)\n",
    "    dict_of_text_lists[party] = list_of_all_party_texts\n",
    "    print('Total docs for party', party, '=', len(list_of_all_party_texts))\n",
    "    list_of_all_texts = list_of_all_texts + list_of_all_party_texts\n",
    "    list_of_all_party_texts = []\n",
    "\n",
    "print('Total documents: ', len(list_of_all_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing three out of every four National Party press release over time:\n",
      "National had 900 press releases\n",
      "National now has 225 press releases\n"
     ]
    }
   ],
   "source": [
    "# Undersample National to improve dataset balance\n",
    "# If re-running, need to run the cell above first!\n",
    "\n",
    "print ('Removing three out of every four National Party press release over time:')\n",
    "print('National had', len(dict_of_text_lists['national']), 'press releases')\n",
    "top_end = len(dict_of_text_lists['national'])\n",
    "              \n",
    "for i in range(0, int(top_end/4)):\n",
    "#for i in range(1, int(top_end/4)):  # Robustness check - try a different quarter\n",
    "#for i in range(2, int(top_end/4)):  # Robustness check - try a different quarter\n",
    "#for i in range(3, int(top_end/4)):  # Robustness check - try a different quarter\n",
    "    del dict_of_text_lists['national'][i]\n",
    "    del dict_of_text_lists['national'][i]        \n",
    "    del dict_of_text_lists['national'][i]        \n",
    "    i += 3\n",
    "    \n",
    "print('National now has', len(dict_of_text_lists['national']), 'press releases')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "Original text: \n",
      "The 2017 campaign of promises demonstrates the old parties in Parliament are de\n",
      "\n",
      " Stripped text:  The   campaign of promises demonstrates the old parties in Parliament are despe\n",
      "\n",
      " After stem: the campaign of promis demonstr the old parti in parliament are desper do anyth \n"
     ]
    }
   ],
   "source": [
    "# Get rid of names (giveaways for the authoring problem) and a few other problematic ('cheat') strings, and stem the text\n",
    "\n",
    "stem_words = []\n",
    "party_list_of_proc_texts = []\n",
    "dict_of_proc_text_lists = {}\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "for party in parties:\n",
    "    party_list_of_proc_texts = []\n",
    "    \n",
    "    # Remove the words in my manual strip list above\n",
    "    for text in dict_of_text_lists[party]:\n",
    "        strip_text = text\n",
    "        for goner in strip_list:\n",
    "            if goner in text:\n",
    "                strip_text = strip_text.replace(goner, ' ')                \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # Apply Snowball stemmer:\n",
    "        stem_words = []\n",
    "        text_words = strip_text.split()\n",
    "        for word in text_words:\n",
    "            stem_word = stemmer.stem(word)\n",
    "            stem_words.append(stem_word)\n",
    "        stem_text = \" \".join(stem_words)\n",
    "        \n",
    "        # Strip a few problematic strings from the stemmed text:\n",
    "        for stemword in strip_from_stemmed:\n",
    "            if stemword in stem_text:\n",
    "                stem_text = stem_text.replace(stemword, ' ')\n",
    "        party_list_of_proc_texts.append(stem_text)\n",
    "        \n",
    "    # Put list of processed party texts in dictionary with party as key\n",
    "    dict_of_proc_text_lists[party] = party_list_of_proc_texts\n",
    "\n",
    "        \n",
    "    \n",
    "print('Example:')    \n",
    "print('Original text:', text[:80])\n",
    "print('\\n', 'Stripped text:', strip_text[:80])\n",
    "print('\\n', 'After stem:' , stem_text[:80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917\n",
      "917\n"
     ]
    }
   ],
   "source": [
    "# Make a list of (unlabelled) processed texts\n",
    "list_of_all_proc_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    procd = dict_of_proc_text_lists[party]\n",
    "    list_of_all_proc_texts = list_of_all_proc_texts + procd\n",
    "    \n",
    "print(len(list_of_all_proc_texts))\n",
    "    \n",
    "# Make a list of party authors that will match up with the texts \n",
    "party_match = []\n",
    "for party in dict_of_proc_text_lists:\n",
    "    for text in dict_of_proc_text_lists[party]:\n",
    "        party_match.append(party)\n",
    "        \n",
    "print(len(party_match))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets:\n",
    "docs_train, docs_test, labels_train, labels_test = model_selection.train_test_split(list_of_all_proc_texts, \n",
    "                                                                                    party_match, \n",
    "                                                                                    test_size = 0.2,\n",
    "                                                                                    stratify = party_match)\n",
    "# Stratify ensures same balance of test as training data\n",
    "print(len(docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.9, min_df = 0.005, stop_words='english', ngram_range=(1, 3))\n",
    "features_train_transf = top_vectorizer.fit_transform(docs_train)\n",
    "features_test_transf  = top_vectorizer.transform(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_feature_names = top_vectorizer.get_feature_names()\n",
    "print('Initial number of features after vectorisation:', len(all_feature_names))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select top x% of most individually useful features\n",
    "top_selector = SelectPercentile(f_classif, percentile=60) \n",
    "top_selector.fit(features_train_transf, labels_train)\n",
    "\n",
    "my_names = np.asarray(top_vectorizer.get_feature_names())[top_selector.get_support()]  #  Feature names (alphabetical)\n",
    "print('New number of features after SelectPercentile:', len(my_names)) # Halved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform and convert to numpy arrays\n",
    "cut_features_train_transf = top_selector.transform(features_train_transf).toarray()\n",
    "cut_features_test_transf  = top_selector.transform(features_test_transf).toarray()\n",
    "\n",
    "# Convert labels from lists to numpy arrays\n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now ready for Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_nb = MultinomialNB(alpha = 0.01)\n",
    "top_fitted     = top_nb.fit(cut_features_train_transf, labels_train)\n",
    "test_predicted = top_nb.predict(cut_features_test_transf) \n",
    "print(classification_report(test_predicted, labels_test, labels = parties))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix')\n",
    "print(metrics.confusion_matrix(labels_test, test_predicted))\n",
    "# {'act': 26, 'green': 22, 'labour': 34, 'maori': 10, 'national': 60, 'nzfirst': 47}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make list of features with highest coefficient values, per class, from most to least important\n",
    "\n",
    "def list_top_features(classifier, feature_names, num_feat):\n",
    "    party_words = {}\n",
    "    counter = 0\n",
    "    top = 'top' + str(num_feat)\n",
    "    for i, label_train in enumerate(parties):          # enumerate loops with an automatic counter (in this case, i)\n",
    "        top = np.argsort(classifier.coef_[i])[::-1][0:num_feat]\n",
    "        list_top = str(', '.join(feature_names[j] for j in top)).split(',')\n",
    "        print(\" \")\n",
    "        print(parties[counter], 'most distinguishing words from most to ' + str(num_feat) + ':')\n",
    "        print(list_top)\n",
    "        party_words[parties[counter]] = list_top  # dict of lists where keys are parties\n",
    "        counter += 1        \n",
    "    return(party_words)\n",
    "\n",
    "\n",
    "\n",
    "# This version also prints out coefficients:\n",
    "\n",
    "def list_top_features_with_coefs(classifier, feature_names, num_feat):\n",
    "    word_nb_coef = {}\n",
    "    party_word_nb_coef = {}\n",
    "    maximum = 0\n",
    "    minimum = 0\n",
    "    \n",
    "    len_feature_names = len(feature_names)\n",
    "    for i in range(6):\n",
    "        print('\\n', parties[i], '\\n')  \n",
    "        diff = classifier.feature_log_prob_[i,:] - np.max(classifier.feature_log_prob_[-i:]) # Only works for NB\n",
    "        \n",
    "        name_diff = {}   \n",
    "        for j in range(len_feature_names):\n",
    "            name_diff[feature_names[j]] = diff[j]\n",
    "            names_diff_sorted = sorted(name_diff.items(), key = op.itemgetter(1), reverse = True)\n",
    "        # Check min coef - any negative?\n",
    "            if diff[j] < minimum:\n",
    "                minimum = diff[j]\n",
    "            else:\n",
    "                pass\n",
    "            if diff[j] > maximum:\n",
    "                maximum = diff[j]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for k in range(num_feat):\n",
    "            print(k, names_diff_sorted[k])\n",
    "            word_nb_coef[names_diff_sorted[k][0]] = names_diff_sorted[k][1] # Dictionary of word coefficients            \n",
    "        party_word_nb_coef[parties[i]] = word_nb_coef    \n",
    "        print ('maximum', maximum, 'minimum', minimum)\n",
    "    return party_word_nb_coef       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_nb_party_words = list_top_features(top_nb, my_names, 200)   \n",
    "party_word_nb_coef = list_top_features_with_coefs(top_nb, my_names, 200) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (party_word_nb_coef['green']['river']) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try a linear SVC for comparison - use the above values for feature selection (min_df and max_df in the Tfidf vectorizer, \n",
    "# percentile in the selector) and only gridsearch the SVC hyperparameter C.\n",
    "\n",
    "text_svc_clf = Pipeline([('vect', TfidfVectorizer(sublinear_tf=True, max_df = 0.9, min_df = 0.005, \n",
    "                                                  stop_words='english', ngram_range=(1, 3))),\n",
    "                         ('selector', SelectPercentile(percentile = 60)),\n",
    "                         ('svc_clf', LinearSVC(class_weight = 'balanced'))])\n",
    "text_svc_clf.fit(docs_train, labels_train) \n",
    "\n",
    "# class_weight: Set the parameter C of class i to class_weight[i]*C for SVC.\n",
    "# If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y \n",
    "# to automatically adjust weights inversely proportional to class frequencies in the input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gridsearch LinearSVC over C\n",
    "svc_parameters = {'svc_clf__C': (0.001, 0.01, 0.1, 1, 10, 100, 1000)}\n",
    "\n",
    "svc_scoring = {'F1':       make_scorer(f1_score, average='weighted'), \n",
    "               'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "gs_svc_clf = GridSearchCV(text_svc_clf, svc_parameters, scoring = svc_scoring, refit = 'F1', cv = 6)\n",
    "gs_svc_clf.fit(docs_train, labels_train)\n",
    "svc_results = gs_svc_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Best score\")\n",
    "print(gs_svc_clf.best_score_)   \n",
    "\n",
    "for param_name in sorted(svc_parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_svc_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use on test set\n",
    "svc_test_predicted = gs_svc_clf.predict(docs_test)\n",
    "\n",
    "# Assess performance:\n",
    "print(classification_report(svc_test_predicted, labels_test, labels = parties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examine top features and see if there is much crossover \n",
    "top_svc = LinearSVC(class_weight = 'balanced', C = 1)\n",
    "top_svc.fit(cut_features_train_transf, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_top_svc_features_with_coefs(classifier, feature_names, num_feat):\n",
    "    len_feature_names = len(feature_names)\n",
    "    word_nb_coef = {}\n",
    "    minimum = 0\n",
    "    maximum = 0\n",
    "    for i in range(6):\n",
    "        print('\\n', parties[i], '\\n')  \n",
    "        coef = classifier.coef_[i,:]  \n",
    "        name_coef = {}   \n",
    "        for j in range(len_feature_names):\n",
    "            name_coef[feature_names[j]] = coef[j]\n",
    "            names_coef_sorted = sorted(name_coef.items(), key = op.itemgetter(1), reverse = True)\n",
    "        # Check min coef - any negative?\n",
    "            if classifier.coef_[i,j] > minimum:\n",
    "                minimum = classifier.coef_[i,j]\n",
    "            else:\n",
    "                pass\n",
    "            if classifier.coef_[i,j] < maximum:\n",
    "                maximum = classifier.coef_[i,j]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for k in range(num_feat):\n",
    "            print(k, names_coef_sorted[k])   \n",
    "        print(\"max:\", maximum, \"min\", minimum)\n",
    "        \n",
    " # SVC coefficients can go negative - squaring is a bad idea.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_svc_party_words = list_top_features(top_svc, my_names, 200)   \n",
    "list_top_svc_features_with_coefs(top_svc, my_names, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_nb_party_words = list_top_features(top_nb, my_names, 200)   \n",
    "#print(type(top_nb_party_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare with top nb:\n",
    "\n",
    "# Calculate crossover between top Naive Bayes model and top SVC model:\n",
    "common_words = {}\n",
    "for party in parties:\n",
    "    if len(top_nb_party_words[party]) == len(top_svc_party_words[party]):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Warning: invalid comparison - the lists are different lengths.\")\n",
    "    common = list(set(top_nb_party_words[party]).intersection(top_svc_party_words[party]))  \n",
    "    print(party, \":\", len(common), \"out of\", len(top_nb_party_words[party]), \"words are the same. (\", \n",
    "          100*len(common)/len(top_svc_party_words[party]), '%)')\n",
    "    common_words[party] = common\n",
    "    print(common)\n",
    "    \n",
    "# I'm more comfortable interpreting the Naive Bayes coefficients so I will use them, but use the cross-over list.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert NB coefficients to suitable number for wordle.net\n",
    "#party_word_nb_coef = list_top_features_with_coefs(top_nb, my_names, 200) \n",
    "\n",
    "party_word_cloud_coef = {}\n",
    "\n",
    "for party in parties:\n",
    "    print('\\n', party, '\\n')\n",
    "    party_word_cloud_coef[party] = {}\n",
    "    for word in common_words[party]:\n",
    "        word = word.strip()\n",
    "        party_word_cloud_coef[party][word] = str(int(100*(party_word_nb_coef[party][word] + 3))) # Make them positive integers\n",
    "        my_string = word + ':' + party_word_cloud_coef[party][word] # Print in format ready to drop into wordle.net\n",
    "        print (my_string)\n",
    "\n",
    "# These are still stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# READ IN NEW (OUT-OF-SAMPLE) TEST DOCUMENTS: LATEST PRESS RELEASES\n",
    "# Create dictionary where key = label (party) and value = list of release text strings.\n",
    "def read_in_new(party, folder):\n",
    "    release_text_list = []\n",
    "    for filename in os.listdir(folder):\n",
    "        full_path = os.path.join(folder, filename)\n",
    "        #print(full_path)\n",
    "        file_obj = open(os.path.join(folder, filename), 'r', encoding='utf8')\n",
    "        content = file_obj.read()\n",
    "        file_obj.close()\n",
    "        release_text_list.append(content)\n",
    "    return(release_text_list)\n",
    "\n",
    "dict_of_new_text_lists = {}\n",
    "list_of_all_new_texts = []\n",
    "list_of_all_new_party_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    new_folder = str(os.path.join('test', party))\n",
    "    print(new_folder)\n",
    "    list_of_all_new_party_texts = read_in_new(party, new_folder)\n",
    "    #print(list_of_all_new_party_texts[0][:100])\n",
    "    dict_of_new_text_lists[party] = list_of_all_new_party_texts\n",
    "    print('Total new docs for party', party, '=', len(list_of_all_new_party_texts))\n",
    "    list_of_all_new_texts = list_of_all_new_texts + list_of_all_new_party_texts\n",
    "    list_of_all_new_party_texts = []\n",
    "\n",
    "print('Total documents: ', len(list_of_all_new_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Strip and stem\n",
    "\n",
    "new_stem_words = []\n",
    "party_list_of_new_proc_texts = []\n",
    "dict_of_new_proc_text_lists = {}\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "for party in parties:\n",
    "    party_list_of_new_proc_texts = []\n",
    "    \n",
    "    # Remove the words in my manual strip list above\n",
    "    for text in dict_of_new_text_lists[party]:\n",
    "        strip_text = text\n",
    "        for goner in strip_list:\n",
    "            if goner in text:\n",
    "                strip_text = strip_text.replace(goner, ' ')                \n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # Apply Snowball stemmer:\n",
    "        new_stem_words = []\n",
    "        text_words = strip_text.split()\n",
    "        for word in text_words:\n",
    "            stem_word = stemmer.stem(word)\n",
    "            new_stem_words.append(stem_word)\n",
    "        stem_text = \" \".join(new_stem_words)\n",
    "        \n",
    "        # Strip a few problematic strings from the stemmed text:\n",
    "        for stemword in strip_from_stemmed:\n",
    "            if stemword in stem_text:\n",
    "                stem_text = stem_text.replace(stemword, ' ')\n",
    "        party_list_of_new_proc_texts.append(stem_text)\n",
    "        \n",
    "    # Put list of processed party texts in dictionary with party as key\n",
    "    dict_of_new_proc_text_lists[party] = party_list_of_new_proc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a list of (unlabelled) processed texts\n",
    "list_of_all_new_proc_texts = []\n",
    "\n",
    "for party in parties:\n",
    "    procd = dict_of_new_proc_text_lists[party]\n",
    "    list_of_all_new_proc_texts = list_of_all_new_proc_texts + procd\n",
    "    \n",
    "print(len(list_of_all_new_proc_texts))\n",
    "    \n",
    "# Make a list of party authors that will match up with the texts \n",
    "new_party_match = []\n",
    "for party in dict_of_new_proc_text_lists:\n",
    "    for text in dict_of_new_proc_text_lists[party]:\n",
    "        new_party_match.append(party)\n",
    "        \n",
    "print(len(new_party_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_test2_transf = top_vectorizer.transform(list_of_all_new_proc_texts)\n",
    "cut_features_test2_transf  = top_selector.transform(features_test2_transf).toarray()\n",
    "print(cut_features_test2_transf.shape[0]) # sparse numpy array. shape[0] = 184, number of docs. shape[1] = 6336, num of features\n",
    "print(cut_features_test2_transf.shape[1]) # sparse numpy array. shape[0] = 184, number of docs. shape[1] = 3801, num of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_new_test = np.array(new_party_match)\n",
    "print(len(labels_new_test))\n",
    "print(features_test2_transf.shape[0]) # sparse numpy array. shape[0] = 184, number of docs. shape[1] = num of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes \n",
    "test_new_predicted_nb = top_nb.predict(cut_features_test2_transf) \n",
    "print(classification_report(test_new_predicted_nb, labels_new_test, labels = parties))\n",
    "print('Confusion Matrix')\n",
    "print(metrics.confusion_matrix(labels_new_test, test_new_predicted_nb))\n",
    "\n",
    "# SVC\n",
    "test_new_predicted_svc = top_svc.predict(cut_features_test2_transf) \n",
    "print(classification_report(test_new_predicted_svc, labels_new_test, labels = parties))\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(metrics.confusion_matrix(labels_new_test, test_new_predicted_svc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
