{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook downloads and saves as text documents press releases FOR SEPTEMBER from the 6 New Zealand political parties likely to be represented in Parliament following the September 2017 General Election: National, Labour, NZ First, Greens, Maori Party, and ACT and saves in a 'test' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_all():\n",
    "    urls   = []\n",
    "    titles = []\n",
    "    all_links = []\n",
    "    slash_links = []\n",
    "    releases_pages = []\n",
    "    return(urls, titles, all_links, slash_links, releases_pages)\n",
    "\n",
    "def write_to_file(textstr, filename):    \n",
    "    text = bytes(textstr, 'utf-8')\n",
    "    file = open(filename,'wb')\n",
    "    file.write(text)\n",
    "    file.close()\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Labour press releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First scrape press release URLs\n",
    "all_links = []\n",
    "\n",
    "# Pages with press release urls on them (5 pages of them from start Sep):\n",
    "releases_pages = ['http://www.labour.org.nz/press_releases']\n",
    "for i in range(2,6):  # (2,6) is September\n",
    "    pg = 'http://www.labour.org.nz/press_releases?page=' + str(i) \n",
    "    releases_pages.append(pg)\n",
    "\n",
    "for each_page in releases_pages:    \n",
    "    r = urllib.request.urlopen(each_page).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    #print(soup.prettify())\n",
    "    \n",
    "    mess = soup.find_all('span', class_=\"read-more\")\n",
    "    for subby in mess:\n",
    "        links = subby.find_all('a')        \n",
    "        for link in links:\n",
    "            relative_url = link.get('href')\n",
    "            relative_url = relative_url.replace('http://www.labour.org.nz', '') # Fix the occasional absolute url\n",
    "            new_link = 'http://www.labour.org.nz' + str(relative_url)\n",
    "            all_links.append(new_link)\n",
    "\n",
    "print(len(all_links))\n",
    "#print(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_labour(urls):\n",
    "    clear_all()\n",
    "    for newurl in urls:\n",
    "        print(newurl)    \n",
    "        filename = newurl[25:] + '.txt'\n",
    "        full_path = os.path.join('test/labour', filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            page      = urllib.request.urlopen(newurl).read()\n",
    "            soup      = BeautifulSoup(page, \"lxml\")\n",
    "         #   print(soup.prettify())\n",
    "            reltext   = soup.find('div', class_ = 'content blog-content').get_text()\n",
    "            if soup.find(class_ = 'byline'):\n",
    "                relbyline = soup.find(class_ = 'byline').get_text()\n",
    "            else:\n",
    "                print('url', newurl, 'has no byline. Add manually.')\n",
    "                relbyline = 'XXX'\n",
    "        #     text = str(relbyline) + str(reltext)        # If you want author and date at start use this\n",
    "            text = str(reltext)        \n",
    "            write_to_file(text, full_path)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "scrape_labour(all_links)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get National Party press releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_all()\n",
    "slash_links = []\n",
    "all_links = []\n",
    "\n",
    "# Pages with press release urls on them:\n",
    "releases_pages = ['https://www.national.org.nz/press']\n",
    "for i in range(2,10):  # (2,10) is Sep (8 onward)\n",
    "    pg = 'https://www.national.org.nz/press?page=' + str(i) \n",
    "    releases_pages.append(pg)\n",
    "\n",
    "for each_page in releases_pages:    \n",
    "    r = urllib.request.urlopen(each_page).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    for link in soup.find_all('a'):\n",
    "        all_links.append(link.get('href'))\n",
    "        \n",
    "for linky in all_links:\n",
    "    if linky:\n",
    "        if len(linky)>36:\n",
    "            if linky[0:36] == 'http://www.facebook.com/share.php?u=':\n",
    "                slash_links.append(linky[36:])\n",
    "            else:   \n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(len(slash_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now scrape similarly to before.\n",
    "\n",
    "def scrape_national(urls):\n",
    "    counter = 0\n",
    "    for newurl in urls:\n",
    "    #    print(newurl)   \n",
    "        filename = newurl[28:] + '.txt'\n",
    "        full_path = os.path.join('test/national', filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            page      = urllib.request.urlopen(newurl).read()\n",
    "            soup      = BeautifulSoup(page, \"lxml\")\n",
    "    #        print(soup.prettify())\n",
    "            reltext   = soup.find('div', class_ = 'content').get_text()\n",
    "            if soup.find(class_ = 'author'):  # Some press releases are missing an author field\n",
    "                relauthor = soup.find(class_ = 'author').get_text()\n",
    "            else:\n",
    "                print('url', newurl, 'has no author field. May need to add manually.')\n",
    "                relauthor = 'XXX'        \n",
    "            if soup.time:  # Some press releases are missing a time field ??\n",
    "                reldate   = soup.time.get_text()\n",
    "            else:\n",
    "                print('url', newurl, 'has no date field. May need to add manually.')\n",
    "                reldate = ' XXX' \n",
    "            text      = str(reltext)\n",
    "#            text      = str(relauthor) + str(reldate) + ' ' + str(reltext)  # Use this if you want author and time\n",
    "            write_to_file(text, full_path)\n",
    "        else:\n",
    "            continue\n",
    "        counter += 1\n",
    "    print(counter,\"press releases saved as text documents.\")   \n",
    "    \n",
    "scrape_national(slash_links)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get NZ First press releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_all()\n",
    "all_links = []\n",
    "\n",
    "# Pages with press release urls on them (47 pages of them for June/July/August):\n",
    "releases_pages = ['http://www.nzfirst.org.nz/news']\n",
    "for i in range(2,8):  # (2,8) is all of Sep\n",
    "    pg = 'http://www.nzfirst.org.nz/news?page=' + str(i) \n",
    "    releases_pages.append(pg)\n",
    "\n",
    "for each_page in releases_pages:    \n",
    "    r = urllib.request.urlopen(each_page).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    #print(soup.prettify())\n",
    "    \n",
    "    mess = soup.find_all('div', class_=\"blog-title-wrap\")\n",
    "    for subby in mess:\n",
    "        messy = subby.find_all('a')\n",
    "        #print(mess)\n",
    "        for link in messy:\n",
    "            relative_url = link.get('href')\n",
    "            new_link = 'http://www.nzfirst.org.nz' + str(relative_url)\n",
    "            all_links.append(new_link)\n",
    "\n",
    "print(len(all_links))\n",
    "#print(all_links[:44])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now scrape similarly to before.\n",
    "''\n",
    "def scrape_nzfirst(urls):\n",
    "    counter = 0\n",
    "    for newurl in urls:\n",
    "        print(newurl)\n",
    "        try:\n",
    "            page = urllib.request.urlopen(newurl).read()\n",
    "        except urllib.error.URLError:\n",
    "            print (\"Page not found!\")    \n",
    "            continue\n",
    "        filename = newurl[26:] + '.txt'\n",
    "        full_path = os.path.join('test/nzfirst', filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            soup      = BeautifulSoup(page, \"lxml\")\n",
    "      #      print(soup.prettify())\n",
    "            if soup.find('div', class_ = 'content wysiwyg'):\n",
    "                reltext = soup.find('div', class_ = \"content wysiwyg\")\n",
    "            else:\n",
    "                print(\"for url \", url, \"I cannot find class content wysiwyg\")\n",
    "            if soup.find(class_ = 'page-tag'):  \n",
    "                relauthor = soup.find(class_ = 'page-tag').get_text()\n",
    "            else:\n",
    "                print('url', newurl, 'has no author field. May need to add manually.')\n",
    "                relauthor = 'unauthored '        \n",
    "            if soup.find('div', class_ = 'meta').get_text():               \n",
    "                reldate   = soup.find('div',class_ = 'meta').get_text()\n",
    "            else:\n",
    "                print('url', newurl, 'has no date field. Add manually.')\n",
    "                reldate = ' XXX' \n",
    "    #         text      = str(relauthor) + str(reldate) + ' ' + reltext.get_text() # Use this if you want author and date\n",
    "            text      = reltext.get_text()\n",
    "\n",
    "            write_to_file(text, full_path)\n",
    "            counter += 1\n",
    "    print(counter,\"NZ First press releases saved as text documents.\")   \n",
    "    \n",
    "scrape_nzfirst(all_links) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get Green party press releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = []\n",
    "\n",
    "# Pages with press release urls on them (10 pages of them for June/July/August):\n",
    "releases_pages = ['https://www.greens.org.nz/media?f[0]=type%3Agreens_press_release']\n",
    "for i in range(1,4):  # (1,7) is all of Sep\n",
    "    pg = 'https://www.greens.org.nz/media?page=' + str(i) + '1&f[0]=type%3Agreens_press_release' \n",
    "    releases_pages.append(pg)\n",
    "\n",
    "for each_page in releases_pages:    \n",
    "    r = urllib.request.urlopen(each_page).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "  #  print(soup.prettify())\n",
    "    mess = soup.find_all(class_=\"field-title\")\n",
    "    for subby in mess:\n",
    "        messy = subby.find_all('a')\n",
    "        for link in messy:\n",
    "            relative_url = link.get('href')\n",
    "            new_link = 'http://www.greens.org.nz' + str(relative_url)\n",
    "            all_links.append(new_link)\n",
    "\n",
    "print(len(all_links))\n",
    "print(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now scrape similarly to before.\n",
    "\n",
    "def scrape_greens(urls):\n",
    "    counter = 0\n",
    "    for newurl in urls: \n",
    "        filename = newurl[25:] + '.txt' # Their urls don't work as file names because of slashes etc\n",
    "        filename = filename.replace('/','') # strip slashes\n",
    "        filename = filename.replace('%','') # strip %\n",
    "        filename = filename[20:]\n",
    "        full_path = os.path.join('test/green', filename)\n",
    "        print(newurl)\n",
    "        try:\n",
    "            page = urllib.request.urlopen(newurl).read()\n",
    "        except urllib.error.URLError:\n",
    "            print (\"Page not found!\")    \n",
    "            continue\n",
    "        soup      = BeautifulSoup(page, \"lxml\")\n",
    "       # print(soup.prettify())\n",
    "        if soup.find('div', class_ = 'field-body'):\n",
    "            reltext = soup.find('div', class_ = \"field-body\")\n",
    "        else:\n",
    "            print(\"for url \", newurl, \"I cannot find content\")\n",
    "            continue\n",
    "        if soup.find(class_ = 'field-posted'):  \n",
    "            relauthor_date = soup.find('div', class_ = 'field-posted').get_text()\n",
    "        else:\n",
    "            print('url', newurl, 'has no author-date (field-posted) field. May need to add manually.')\n",
    "            relauthor_date = 'unauthored '        \n",
    "#        text      = str(relauthor_date) + ' ' + reltext.get_text()\n",
    "        text      = reltext.get_text()    # Version without author and date\n",
    "        \n",
    "        if not os.path.exists(filename):\n",
    "            write_to_file(text, full_path)\n",
    "        else:\n",
    "            continue\n",
    "        counter += 1\n",
    "    print(counter, \"Greens press releases saved as text documents.\")   \n",
    "    \n",
    "scrape_greens(all_links) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Get Maori party press releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Their press releases start on their main page and the urls are ridiculous thereafter\n",
    "\n",
    "all_links = []\n",
    "\n",
    "# Pages with press release urls on them (xx pages of them for June/July/August):\n",
    "releases_pages = ['http://www.maoriparty.org']\n",
    "#for i in range(2,3):  # Actually all of Sep are on the home page\n",
    "#    pg = 'http://www.maoriparty.org/?fp=t%2F0pb2vva74axufxqvxziwymplu1c0katfo8mi5qhh3jupuv2btt7auhgr2j5o%2Fnyzsa7ra4dg2ukkxsikmqxg%3D%3D&page=' + str(i) + '&poru=wnycig1t%2Ba24%2Fpnhlfz7yto%2Faklt5encfsqsueywx7hlehvjtjzlv2zerv7z2haa&prvtof=j9ejw%2Falc85a68hgs2xyeg76zfbcphy1bxbsk8ororm%3D' + str(i) \n",
    "#    releases_pages.append(pg)\n",
    "\n",
    "for each_page in releases_pages:    \n",
    "    r = urllib.request.urlopen(each_page).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    #print(soup.prettify())\n",
    "    \n",
    "    mess = soup.find_all(class_=\"blog\")\n",
    "    for subby in mess:\n",
    "        messy = subby.find_all('a')\n",
    "        for link in messy:\n",
    "            relative_url = link.get('href')\n",
    "            new_link = 'http://www.maoriparty.org' + str(relative_url)\n",
    "            all_links.append(new_link)\n",
    "\n",
    "all_links = list(set(all_links))      # There is duplication in the list.      \n",
    "print(len(all_links))\n",
    "print(all_links)  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now scrape similarly to before.\n",
    "\n",
    "def scrape_maori(urls):\n",
    "    counter = 0\n",
    "    for newurl in urls: \n",
    "        print(newurl)\n",
    "        filename = newurl[26:] + '.txt'         \n",
    "        full_path = os.path.join('test/maori', filename)\n",
    "        if not os.path.exists(full_path):\n",
    "\n",
    "            try:\n",
    "                page = urllib.request.urlopen(newurl).read()\n",
    "            except urllib.error.URLError:\n",
    "                print (\"Page not found!\")    \n",
    "                continue\n",
    "            soup      = BeautifulSoup(page, \"lxml\")\n",
    "           # print(soup.prettify())\n",
    "            if soup.find('div', class_ = 'content'):\n",
    "                reltext = soup.find('div', class_ = \"content\")\n",
    "            else:\n",
    "                print(\"for url \", url, \"I cannot find content\")\n",
    "\n",
    "            text      =  reltext.get_text()\n",
    "\n",
    "            write_to_file(text, full_path)\n",
    "        else:\n",
    "            continue\n",
    "        counter += 1\n",
    "    print(counter, \"Maori Party press releases saved as text documents.\")   \n",
    "    \n",
    "scrape_maori(all_links) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Get ACT party press releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_all()\n",
    "\n",
    "all_links = []\n",
    "# Pages with press release urls on them (18 pages of them for June/July/August):\n",
    "releases_pages = ['http://act.org.nz/news'] # 1st page\n",
    "for i in range(2,6):  # (2,6) is all of Sep\n",
    "    pg = 'http://act.org.nz/news/page/' + str(i) + '/' \n",
    "    releases_pages.append(pg)\n",
    "\n",
    "for each_page in releases_pages:    \n",
    "    r = urllib.request.urlopen(each_page).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "  #  print(soup.prettify())\n",
    "    mess = soup.find_all(class_=\"read-more\")\n",
    "    for subby in mess:\n",
    "        messy = subby.find_all('a')  # It's a full link, not relative\n",
    "        for link in messy:\n",
    "            new_link = link.get('href')\n",
    "            all_links.append(new_link)\n",
    "\n",
    "print(len(all_links))\n",
    "print(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now scrape similarly to before.\n",
    "\n",
    "def scrape_act(urls):\n",
    "    counter = 0\n",
    "    for newurl in urls: \n",
    "        print(newurl)\n",
    "        filename = newurl[18:] + '.txt' \n",
    "        filename = filename.replace('/','') # strip slashes\n",
    "        full_path = os.path.join('test/act', filename)\n",
    "        if not os.path.exists(full_path):\n",
    "            try:\n",
    "                page = urllib.request.urlopen(newurl).read()\n",
    "            except urllib.error.URLError:\n",
    "                print (\"Page not found!\")    \n",
    "                continue\n",
    "            soup      = BeautifulSoup(page, \"lxml\")\n",
    "            #print(soup.prettify())\n",
    "\n",
    "            if soup.find('div', class_ = 'entry-content'):\n",
    "                reltext = soup.find('div', class_ = \"entry-content\")\n",
    "                temptext = reltext.get_text()\n",
    "                temptext = temptext.replace('Facebook0Twitter', '')\n",
    "            else:\n",
    "                print(\"for url \", url, \"I cannot find class entry-content\")\n",
    "\n",
    "            if soup.find(class_ = 'fn'):  \n",
    "                relauthor = soup.find(class_ = 'fn').get_text()\n",
    "                relauthor = relauthor + ' '\n",
    "            else:\n",
    "                print('url', newurl, 'has no author field. May need to add manually.')\n",
    "                relauthor = 'unauthored '      \n",
    "\n",
    "            if soup.find('time', class_ = 'updated'):               \n",
    "                reldate   = soup.find('time', class_ = 'updated').get_text()\n",
    "            else:\n",
    "                print('url', newurl, 'has no date field. Add manually.')\n",
    "                reldate = ' XXX' \n",
    "            text      = str(relauthor) + str(reldate) + ' ' + temptext  # Version with date and author\n",
    "            text      = temptext\n",
    "\n",
    "            write_to_file(text, full_path)\n",
    "        else:\n",
    "            continue\n",
    "        counter += 1\n",
    "    print(counter, \"ACT press releases saved as text documents.\")   \n",
    "    \n",
    "scrape_act(all_links) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The earlier data did include some releases from early September. Delete double-ups\n",
    "\n",
    "parties = ['act', 'green', 'labour', 'maori', 'national', 'nzfirst']\n",
    "\n",
    "for party in parties:\n",
    "    new_folder = os.path.join('test', party)\n",
    "    print('new_folder:', new_folder)\n",
    "    for release in os.listdir(new_folder):\n",
    "        new_path = os.path.join('test', party,  release)\n",
    "        print ('new_path:', new_path)\n",
    "        old_path = os.path.join(party, release)\n",
    "        if os.path.exists(old_path):\n",
    "            os.remove(new_path)\n",
    "            print('deleted file', new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
